{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-528f0c483209>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfam_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m \u001b[0mfile_dicts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_WGS_fibroblast_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[0mSubj_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile_dicts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[0mUUID_Dicts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile_dicts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-528f0c483209>\u001b[0m in \u001b[0;36mget_WGS_fibroblast_dict\u001b[1;34m()\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_WGS_fibroblast_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m     \u001b[0m_Location\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m     \u001b[0m_Data_Base_File\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Location\u001b[0m\u001b[1;33m+\u001b[0m \u001b[1;34m\"/data/Cardips_12_9_15_Active.csv\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mtable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_Data_Base_File\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: global name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import sys\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "\n",
    "\n",
    "fn = '/frazer01/projects/CARDIPS/analysis/cardips-cnv-analysis/private_output/Table_S1_CNV_SNP_Stats/Table_S1_Subject_info.pkl'\n",
    "info_df = pd.read_pickle(fn)\n",
    "\n",
    "def datestring(month=True, day=True, year=True, hour=False, minute=False):\n",
    "    d = datetime\n",
    "    \n",
    "    y = str(d.datetime.now().timetuple().tm_year)\n",
    "    m =  str(d.datetime.now().timetuple().tm_mon)\n",
    "    D = str(d.datetime.now().timetuple().tm_mday)\n",
    "    h = str(d.datetime.now().timetuple().tm_hour)\n",
    "    mins = str(d.datetime.now().timetuple().tm_min)\n",
    "    h_original = h\n",
    "    \n",
    "    if int(month) in range(0,10):\n",
    "        m = \"0\" + m\n",
    "    \n",
    "    if hour == True:\n",
    "        if int(h) > 12:\n",
    "            h = str(int(h) - 12) + 'PM'\n",
    "        \n",
    "        elif int(h)<= 12:\n",
    "            h = h + 'AM'\n",
    "    \n",
    "    dt_array = [y, m, D, h, mins]\n",
    "    t_f_array = [year, month, day, hour, minute]\n",
    "    \n",
    "    Date_Array = []\n",
    "    for s, i, in zip(dt_array, t_f_array):\n",
    "        if i == True:\n",
    "            Date_Array.append(s)\n",
    "    \n",
    "    return '_'.join(Date_Array)\n",
    "    \n",
    "normal_Chrs = range(1,23)\n",
    "normal_Chrs.extend(['X','Y'])\n",
    "normal_Chrs = [str(x) for x in normal_Chrs]\n",
    "\n",
    "autosomes =[str(x) for x in range(1,23)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Gets the Uncensored stuff\n",
    "def get_WGS_fibroblast_dict():\n",
    "\n",
    "    _Location = os.path.dirname(os.path.abspath(__file__))\n",
    "    _Data_Base_File = _Location+ \"/data/Cardips_12_9_15_Active.csv\"\n",
    "    table = pd.read_csv(_Data_Base_File)\n",
    "    filedict = table.set_index('ID')['Subject'].to_dict()\n",
    "    UUID_dict = table.set_index('Subject')['ID'].to_dict()\n",
    "    return [filedict, UUID_dict]\n",
    "\n",
    "def iPSC_Dicts():\n",
    "    _Location = os.path.dirname(os.path.abspath(__file__))\n",
    "    _Data_Base_File = _Location+ \"/data/CARDiPS_iPSC_WGS.csv\"\n",
    "    table = pd.read_csv(_Data_Base_File)\n",
    "    filedict = table.set_index('ID')['Subject'].to_dict()\n",
    "    UUID_dict = table.set_index('Subject')['ID'].to_dict()\n",
    "    return [filedict, UUID_dict]\n",
    "\n",
    "def sex_Dicts():\n",
    "    _Location = os.path.dirname(os.path.abspath(__file__))\n",
    "    _Data_Base_File = _Location+ \"/data/Whole_Cardips.csv\"\n",
    "    table = pd.read_csv(_Data_Base_File)\n",
    "    sex_dict = table.set_index('UUID')['Sex'].to_dict()\n",
    "    return sex_dict\n",
    "\n",
    "def fam_Dicts():\n",
    "    _Location = os.path.dirname(os.path.abspath(__file__))\n",
    "    _Data_Base_File = _Location+ \"/data/DB_Dumps/2015_01_06/subject_family.tsv\"\n",
    "    table = pd.read_csv(_Data_Base_File)\n",
    "    fam_dict = table.set_index('name')['id'].to_dict()\n",
    "    return fam_dict\n",
    "\n",
    "file_dicts = get_WGS_fibroblast_dict()\n",
    "Subj_dict = file_dicts[1]\n",
    "UUID_Dicts = file_dicts[0]\n",
    "\n",
    "iPSC_Dicts=iPSC_Dicts()\n",
    "Subj_dict_ipsc= iPSC_Dicts[1]\n",
    "UUID_Dicts_ipsc = iPSC_Dicts[0]\n",
    "sex_dict = sex_Dicts()\n",
    "\n",
    "sex_frame = pd.Series(sex_dict)\n",
    "males = [i for i in UUID_Dicts.keys() if i in sex_frame[sex_frame=='M'].index.tolist()]\n",
    "females =[i for i in UUID_Dicts.keys()if i in sex_frame[sex_frame=='F'].index.tolist()]\n",
    "\n",
    "\n",
    "sample_info_fn = '/frazer01/home/djakubosky/bin/DB_Dumps/2015_01_06/data_wgs.tsv'\n",
    "sample_frame = pd.read_table(sample_info_fn)\n",
    "cell_type_dict = sample_frame[sample_frame['id'].isin(UUID_Dicts.keys())].set_index('id').to_dict()['cell']\n",
    "cell_type_frame = sample_frame[sample_frame['id'].isin(UUID_Dicts.keys())]\n",
    "\n",
    "fibroblasts =  [i for i in UUID_Dicts.keys() if i in cell_type_frame[cell_type_frame.cell=='Fibroblast']['id'].tolist()]\n",
    "blood =  [i for i in UUID_Dicts.keys() if i in cell_type_frame[cell_type_frame.cell=='Blood']['id'].tolist()]\n",
    "\n",
    "\n",
    "\n",
    "rna_seq = pd.read_table('/projects/CARDIPS/analysis/cardips-ipsc-eqtl/output/input_data/rnaseq_metadata.tsv')\n",
    "eqtl_samples = rna_seq[rna_seq['in_eqtl']==True].subject_id.values.tolist()\n",
    "\n",
    "subject_info = pd.read_table('/frazer01/home/djakubosky/bin/DB_Dumps/2015_01_06/subject_subject.tsv')\n",
    "subj_info_274 = subject_info[subject_info.name.isin(Subj_dict.keys())]\n",
    "\n",
    "\n",
    "def get_twins_families(df, UUIDS, subject_dictionary):\n",
    "    \"\"\" \n",
    "    df= subject-subject table from CARDiPS database\n",
    "    output = list of lists for families and twins that are present in \n",
    "    list of CARDiPS UUIDS provided \n",
    "    \"\"\"\n",
    "    subject_dictionary=Subj_dict\n",
    "    subject_info = df \n",
    "    subj_info_274 = subject_info[subject_info.name.isin(UUIDS)]\n",
    "    \n",
    "    ID_SUBJ = dict(zip(subject_info.id, subject_info.name))\n",
    "    \n",
    "    Families=[]\n",
    "    Twins=[]\n",
    "    for x,y,z,t,inf in zip(subj_info_274.id, subj_info_274.father_id, subj_info_274.mother_id, \n",
    "                               subj_info_274.twin_id, subj_info_274.relation):\n",
    "        MZ=False\n",
    "        \n",
    "        if type(y)==str and type(z)==str:\n",
    "            Families.append([x,y,z])\n",
    "        try:\n",
    "            tw = inf.split(\" \")\n",
    "            if tw[0]==\"MZ\" and tw[2]==\"to\":\n",
    "                MZ=True\n",
    "        except:\n",
    "            MZ=False\n",
    "        \n",
    "        if MZ==True:\n",
    "            if type(t)==str:\n",
    "                if len(Twins)>0:\n",
    "                \n",
    "                    if t in np.array(Twins).flatten():\n",
    "                        pass\n",
    "                    \n",
    "                    else:\n",
    "                        Twins.append([x,t])\n",
    "        \n",
    "                if len(Twins)==0:\n",
    "                    Twins.append([x,t])\n",
    "    for x in Families:\n",
    "        for ids in x:\n",
    "            z = subject_info[subject_info.id==ids].name.values\n",
    "            z = np.asscalar(z)\n",
    "            try:\n",
    "                t = Subj_dict[z]\n",
    "            except:\n",
    "                try:\n",
    "                    Families.remove(x)\n",
    "                except:\n",
    "                    pass\n",
    "    for x in Twins:\n",
    "        for ids in x:\n",
    "            z = subject_info[subject_info.id==ids].name.values\n",
    "            z = np.asscalar(z)\n",
    "            try:\n",
    "                t = Subj_dict[z]\n",
    "            except:\n",
    "                try:\n",
    "                    Twins.remove(x)\n",
    "                except:\n",
    "                    pass\n",
    "    \n",
    "    # Convert to WGS UUIDS and subj_ID for keys\n",
    "    iv = [Families, Twins]\n",
    "    fam_out = {}\n",
    "    twin_out = {}\n",
    "    dicts = [fam_out, twin_out]\n",
    "    for l, y in zip(iv, dicts):\n",
    "        for item in l:\n",
    "            y['_'.join([ID_SUBJ[i] for i in item])] = [Subj_dict[ID_SUBJ[i]] for i in item]\n",
    "    return dicts\n",
    "\n",
    "Families, Twins = get_twins_families(subject_info, Subj_dict.keys(), Subj_dict)\n",
    "\n",
    "def get_full_families():\n",
    "    fn = '/frazer01/home/djakubosky/bin/DB_Dumps/2015_01_06/subject_subject.tsv'\n",
    "    info_df = pd.read_table(fn)\n",
    "    info_df = info_df[info_df['name'].isin(Subj_dict.keys())]\n",
    "    \n",
    "    id_uuid = {}\n",
    "    for i,s in zip(info_df['id'], info_df['name']):\n",
    "        id_uuid[i]= Subj_dict[s]\n",
    "    \n",
    "    family_dictionary = {}\n",
    "    for n, df in info_df.groupby('family_id'):\n",
    "        family_members = df.name.apply(lambda x: Subj_dict[x]).tolist()\n",
    "        family_dictionary[n] = family_members\n",
    "    return id_uuid, family_dictionary\n",
    "\n",
    "id_uuid, family_dict_full = get_full_families()\n",
    "\n",
    "\n",
    "\n",
    "parent_dict = {}\n",
    "for i, l in info_df.iterrows():\n",
    "    father_id = l.father_id\n",
    "    mother_id = l.mother_id\n",
    "    \n",
    "    has_both = True\n",
    "    \n",
    "    for p in [father_id, mother_id]:\n",
    " \n",
    "        if type(p) <> str:\n",
    "            has_both=False\n",
    "    \n",
    "    if has_both==True:\n",
    "        try:\n",
    "            parent_dict[CM.id_uuid[father_id]]=CM.id_uuid[mother_id]\n",
    "            parent_dict[CM.id_uuid[mother_id]]=CM.id_uuid[father_id]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "fam_dict = {}\n",
    "for x, i in info_df.iterrows():\n",
    "    ext = i.family_ext_name\n",
    "    wgs_uuid = i.WGS_UUID\n",
    "    \n",
    "    fam_dict[wgs_uuid] = ext\n",
    "    \n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "ext_parent_dict = defaultdict(list)\n",
    "for i in parent_dict.keys():\n",
    "    ext = fam_dict[i]\n",
    "    \n",
    "    ext_parent_dict[ext].append(i)\n",
    "\n",
    "    \n",
    "    \n",
    "def save_file_private(var_name, df, folder_loc, alt_name = False, pickle= True,  csv = True, \n",
    "                      sep = \"\\t\", raw_sep = r\"\\t\", csv_suffix = '.tsv', rewrite=False, return_record_fn = False,\n",
    "                      print_records =True, print_only_csv = False, print_only_pickle=False,\n",
    "                      print_vars_recorded_loc=True,**kwargs):\n",
    "    \"\"\"  \n",
    "    purpose:\n",
    "    save a pandas dataframe as pickle and tsv (or any combination thereof), while logging the save to a file\n",
    "    with the needed code to load the variable elsewhere\n",
    "    print the location of saves and commands to load them (by default)\"\"\"\n",
    "    \n",
    "\n",
    "    date_str = datestring(hour = True)\n",
    "    record_file = os.path.join(folder_loc, 'load_saved_nb_variables.py')\n",
    "    pickle_record_file = os.path.join(folder_loc, 'load_pickled_nb_variables.py')\n",
    "    \n",
    "    if return_record_fn == True:\n",
    "        \n",
    "        for f in [record_file, pickle_record_file]:\n",
    "            if os.path.exists(f):\n",
    "                print \"# Record File Exists: {}\".format(f)\n",
    "    \n",
    "    \n",
    "    rw_kw = 'a'\n",
    "    if not os.path.exists(record_file):\n",
    "        rw_kw = 'w'\n",
    "        \n",
    "    elif rewrite == True:\n",
    "        rw_kw = 'w'\n",
    "    else:\n",
    "        rw_kw = 'a'\n",
    "        \n",
    "    if alt_name==False:\n",
    "        file_name = var_name\n",
    "    else:\n",
    "        file_name = alt_name\n",
    "    \n",
    "    \n",
    "    with open(record_file, rw_kw) as F:\n",
    "        with open(pickle_record_file, rw_kw) as P:\n",
    "            \n",
    "            out_fns = []\n",
    "            out_pickles = []\n",
    "            out_pickles.append(\"# \" + date_str)\n",
    "            out_fns.append(\"# \" + date_str)\n",
    "        \n",
    "            fn = os.path.join(folder_loc, file_name)\n",
    "            pickle_fn = fn + '.pkl'\n",
    "            csv_fn = fn + csv_suffix\n",
    "            csv_load = \"{} = pd.read_csv('{}', sep='{}')\".format(var_name, csv_fn, raw_sep)\n",
    "            pickle_load = \"{} = pd.read_pickle('{}')\".format(var_name, pickle_fn)\n",
    "\n",
    "            if csv == True:\n",
    "                df.to_csv(csv_fn, sep=sep, **kwargs)\n",
    "                out_fns.append(csv_load)\n",
    "                \n",
    "            if pickle==True:\n",
    "                df.to_pickle(pickle_fn)\n",
    "                \n",
    "                out_fns.append(pickle_load)\n",
    "                out_pickles.append(pickle_load)\n",
    "                \n",
    "                P.write(\"\\n\".join(out_pickles))\n",
    "                \n",
    "            \n",
    "            if print_records == True:\n",
    "                if not print_only_csv:\n",
    "                    if pickle == True:\n",
    "                        print pickle_load\n",
    "                if not print_only_pickle:\n",
    "                    \n",
    "                    if csv == True:\n",
    "                        print csv_load\n",
    "\n",
    "                else:\n",
    "                    print ''\n",
    "            \n",
    "            out_string =\"\\n\".join(out_fns)\n",
    "            \n",
    "            F.write(out_string)\n",
    "    \n",
    "    if print_vars_recorded_loc ==True:\n",
    "        \n",
    "    \n",
    "        print '# all vars recorded: ' + record_file\n",
    "        print '# pickled vars recorded:' + pickle_record_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_dataframe(var_name, df, folder_loc, alt_name = False, pickle= True,  csv = True, \n",
    "                      sep = \"\\t\", raw_sep = r\"\\t\", csv_suffix = '.tsv', rewrite=False, return_record_fn = False,\n",
    "                      print_records =True, print_only_csv = False, print_only_pickle=False, print_fn_eq = False,\n",
    "                      print_vars_recorded_loc=True,reset_index=False,**kwargs):\n",
    "    \"\"\"  \n",
    "    purpose:\n",
    "    save a pandas dataframe as pickle and tsv (or any combination thereof), while logging the save to a file\n",
    "    with the needed code to load the variable elsewhere\n",
    "    print the location of saves and commands to load them (by default)\"\"\"\n",
    "    \n",
    "\n",
    "    date_str = datestring(hour = True)\n",
    "    record_file = os.path.join(folder_loc, 'load_saved_nb_variables.py')\n",
    "    pickle_record_file = os.path.join(folder_loc, 'load_pickled_nb_variables.py')\n",
    "    \n",
    "    if return_record_fn == True:\n",
    "        \n",
    "        for f in [record_file, pickle_record_file]:\n",
    "            if os.path.exists(f):\n",
    "                print \"# Record File Exists: {}\".format(f)\n",
    "    \n",
    "    \n",
    "    rw_kw = 'a'\n",
    "    if not os.path.exists(record_file):\n",
    "        rw_kw = 'w'\n",
    "        \n",
    "    elif rewrite == True:\n",
    "        rw_kw = 'w'\n",
    "    else:\n",
    "        rw_kw = 'a'\n",
    "        \n",
    "    if alt_name==False:\n",
    "        file_name = var_name\n",
    "    else:\n",
    "        file_name = alt_name\n",
    "    \n",
    "    \n",
    "    with open(record_file, rw_kw) as F:\n",
    "        with open(pickle_record_file, rw_kw) as P:\n",
    "            \n",
    "            out_fns = []\n",
    "            out_pickles = []\n",
    "            out_pickles.append(\"# \" + date_str)\n",
    "            out_fns.append(\"# \" + date_str)\n",
    "        \n",
    "            fn = os.path.join(folder_loc, file_name)\n",
    "            pickle_fn = fn + '.pkl'\n",
    "            csv_fn = fn + csv_suffix\n",
    "            csv_load = \"{} = pd.read_csv('{}', sep='{}')\".format(var_name, csv_fn, raw_sep)\n",
    "            pickle_load = \"{} = pd.read_pickle('{}')\".format(var_name, pickle_fn)\n",
    "            \n",
    "            csv_fn_eq = \"fn = '{}'\".format(csv_fn)\n",
    "\n",
    "            if csv == True:\n",
    "                \n",
    "                if reset_index ==True:\n",
    "                    df.reset_index().to_csv(csv_fn, sep=sep, **kwargs)\n",
    "                    out_fns.append(csv_load)\n",
    "                else:\n",
    "\n",
    "                    df.to_csv(csv_fn, sep=sep, **kwargs)\n",
    "                    out_fns.append(csv_load)\n",
    "\n",
    "                \n",
    "            if pickle==True:\n",
    "                df.to_pickle(pickle_fn)\n",
    "                \n",
    "                out_fns.append(pickle_load)\n",
    "                out_pickles.append(pickle_load)\n",
    "                \n",
    "                P.write(\"\\n\".join(out_pickles))\n",
    "                \n",
    "            \n",
    "            if print_records == True:\n",
    "                if not print_only_csv:\n",
    "                    if pickle == True:\n",
    "                        print pickle_load\n",
    "                if not print_only_pickle:\n",
    "                    \n",
    "                    if csv == True:\n",
    "                        print csv_load\n",
    "                    if print_fn_eq:\n",
    "                        print csv_fn_eq\n",
    "\n",
    "                else:\n",
    "                    print ''\n",
    "            \n",
    "            out_string =\"\\n\".join(out_fns)\n",
    "            \n",
    "            F.write(out_string)\n",
    "    \n",
    "    if print_vars_recorded_loc ==True:\n",
    "        \n",
    "    \n",
    "        print '# all vars recorded: ' + record_file\n",
    "        print '# pickled vars recorded:' + pickle_record_file\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
