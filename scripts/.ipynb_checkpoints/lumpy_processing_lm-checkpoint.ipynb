{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from subprocess import call\n",
    "import subprocess\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "import copy \n",
    "\n",
    "\n",
    "import itertools\n",
    "import tempfile\n",
    "import six\n",
    "import networkx as nx\n",
    "import scipy.stats as stats\n",
    "import argparse\n",
    "\n",
    "import datetime\n",
    "import gzip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def find_header_end(fn):\n",
    "    \"\"\" find header end line number of vcf or gzipped vcf\"\"\"\n",
    "\n",
    "    if fn.split('.').pop() == 'gz':\n",
    "        import gzip\n",
    "        F = gzip.open(fn, 'rU')\n",
    "    else:\n",
    "        F = open(fn, 'rU')\n",
    "\n",
    "\n",
    "    count = 0\n",
    "    for line in F:\n",
    "        count +=1\n",
    "        try: \n",
    "            spl = line.split('\\t')\n",
    "            spl0 = spl[0] \n",
    "            if spl[0]==\"#CHROM\":\n",
    "                F.close()\n",
    "                return count\n",
    "            if count > 2000:\n",
    "                F.close()\n",
    "                return 'incomplete or missing header, or very long header'\n",
    "        except:\n",
    "            break\n",
    "\n",
    "    F.close()\n",
    "\n",
    "def parse_info_col(t, lab, type_out = str, alternative = 'None'):\n",
    "    t = t.split(';')\n",
    "    # filter out the tags with no keyword, if any\n",
    "    t = [i for i in t if i.find('=') != -1]\n",
    "    cols = [i.split('=')[0] for i in t]\n",
    "    try:\n",
    "        vals = [i.split('=')[1] for i in t]    \n",
    "    except:\n",
    "        return \"PARSE_ERROR\"\n",
    "    try:\n",
    "        ind = cols.index(lab)\n",
    "        v = vals[ind]\n",
    "        \n",
    "        try:\n",
    "            return type_out(v)\n",
    "        except:\n",
    "            return alternative\n",
    "    except:\n",
    "        return 'Column_Not_Present'\n",
    "\n",
    "def filter_inv_BND(x):\n",
    "    \"\"\" apply filtering similar to GTeX on these \n",
    "    variant types (making sure the required amount of evidence is present)\"\"\"\n",
    "    SU = int(x.SU)\n",
    "    PE = int(x.PE)\n",
    "    SR = int(x.SR)\n",
    "    Qual = float(x.MSQ)\n",
    "    svtype = x.SVTYPE\n",
    "    \n",
    "\n",
    "    \n",
    "    if svtype in ['BND', 'INV']:\n",
    "        if Qual < 100:\n",
    "            return True    \n",
    "\n",
    "        if svtype == 'INV':\n",
    "            tot = PE + SR\n",
    "            for s in [PE,SR]:\n",
    "                frac = s/tot\n",
    "                if frac < 0.1:\n",
    "                    return True\n",
    "                \n",
    "        \n",
    "        if svtype == 'BND':\n",
    "            tot = PE + SR\n",
    "            for s in [PE,SR]:\n",
    "                frac = s/tot\n",
    "                if frac < 0.25:\n",
    "                    return True\n",
    "        return False\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def bp_precision(x):\n",
    "    beg, end =[int(i) for i in x.split(',')]\n",
    "    interval = abs(end - beg)\n",
    "    return interval\n",
    "\n",
    "\n",
    "def SVLEN_convert(x):\n",
    "    try:\n",
    "        return np.abs(int(x))\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def parse_format_col(t, col, lab, type_out = str, alternative = 'None', change_dict = False):\n",
    "    \"\"\" use the format column to pull out info of a specific VCF record\n",
    "    in a genotyping matrix (unparsed)\"\"\"\n",
    "    \n",
    "    f= t['FORMAT']\n",
    "    c = t[col]\n",
    "    c  = c.split(':')\n",
    "    f = f.split(':')\n",
    "    \n",
    "    try:\n",
    "        ind = f.index(lab)\n",
    "        v = c[ind]\n",
    "        try:\n",
    "            if change_dict:\n",
    "                try:\n",
    "                    v = change_dict.get(v, v)\n",
    "                except:\n",
    "                    return 'ERROR'\n",
    "            \n",
    "            return type_out(v)\n",
    "        except:\n",
    "            return alternative\n",
    "            \n",
    "    except:\n",
    "        return 'Column_Not_Present'\n",
    "    \n",
    "\n",
    "def parse_format_mod(t, col, lab, except_out = 'None'):\n",
    "    \"\"\" use the format column to pull out info of a specific VCF record\n",
    "    in a genotyping matrix (unparsed)\"\"\"\n",
    "    \n",
    "    f= t['FORMAT']\n",
    "    c = t[col]\n",
    "    c  = c.split(':')\n",
    "    f = f.split(':')\n",
    "    \n",
    "    try:\n",
    "        ind = f.index(lab)\n",
    "        v = c[ind]\n",
    "        return v\n",
    "    except:\n",
    "        return except_out\n",
    "\n",
    "\n",
    "def coord_extract(df,chrom, start, end, contained = True):\n",
    "    \n",
    "    if contained:\n",
    "        return df[(df.Chr==chrom) & (df.POS >= start) & (df.END <= end)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_annotations_extract_gts(fn, out_dir, unrelated_samples, non_ipsc_samples, sex_dict):\n",
    "    \n",
    "    header_end = find_header_end(fn)\n",
    "    \n",
    "    count = 0\n",
    "    progress_level = 0\n",
    "    progress_increment = 50000\n",
    "\n",
    "    \n",
    "    \n",
    "    if fn.split('.').pop() == 'gz':\n",
    "        F = gzip.open(fn, 'rU')\n",
    "    else:\n",
    "        F = open(fn, 'rU')\n",
    "    \n",
    "    fn_annotated_vcf = os.path.join(out_dir, 'speedseq_output_annot.vcf')\n",
    "    \n",
    "    gt_file = open(os.path.join(out_dir, 'lumpy_gt.tsv'), 'w')\n",
    "    ab_file = open(os.path.join(out_dir, 'lumpy_ab.tsv'), 'w')\n",
    "    info_file = open(os.path.join(out_dir, 'lumpy_info.tsv'), 'w')\n",
    "    annotated_vcf = open(os.path.join(out_dir, 'speedseq_output_annot.vcf'), 'w')\n",
    "    \n",
    "    gt_filt_file = open(os.path.join(out_dir, 'lumpy_gt_filt.tsv'), 'w')\n",
    "    ab_filt_file = open(os.path.join(out_dir, 'lumpy_ab_filt.tsv'), 'w')\n",
    "    \n",
    "    info_filt_file = open(os.path.join(out_dir, 'lumpy_info_filt.tsv'), 'w')\n",
    "    \n",
    "    chroms = [str(i) for i in range(1,23)] + ['X', 'Y']\n",
    "    \n",
    "    for line in F:\n",
    "        \n",
    "        if progress_level == progress_increment:\n",
    "            d = datetime.datetime.now()\n",
    "            ts = d.strftime('%D- %H:%M')\n",
    "            print \"processed {} variants {}\".format(count, ts)\n",
    "            progress_level = 0\n",
    "        \n",
    "        \n",
    "        line = line.rstrip()\n",
    "                         \n",
    "        lin_spl = line.split('\\t')\n",
    "                         \n",
    "        if count < header_end-1:\n",
    "            annotated_vcf.write(line + '\\n')\n",
    "            \n",
    "                         \n",
    "        if count == header_end-1:\n",
    "            # add filter information into the header\n",
    "            lines = (['##FILTER=<ID=ac0,Description=\"no alternative alleles genotyped at this site\">'] + \n",
    "         ['##FILTER=<ID=OTHER_CONTIG,Description=\"not on autosomes or sex chroms\">'] +\n",
    "         ['##FILTER=<ID=SHORT_LENGTH,Description=\"variant smaller than 50bp\">'] + \n",
    "         ['##FILTER=<ID=SHORT_DEL,Description=\"del less than 418bp without SR\">'] +\n",
    "         ['##FILTER=<ID=MSQ,Description=\"MSQ score below 90 for BND or INV, below 100 for DUP, below 20 for DEL and MEI\">'] +\n",
    "         ['##FILTER=<ID=NMissing,Description=\"at least 10% of samples have a missing GT (iPSCORE fib/blood + HipSci fib)>'] + \n",
    "         ['##FILTER=<ID=PE/SR,Description=\"low PE/SR ratio for BND or INV\">'] +\n",
    "                     ['##FILTER=<ID=QUAL,Description=\"quality score below 100 for BND or INV\">'] + \n",
    "                     ['##FILTER=<ID=iPSC,Description=\"present only in iPSC\">'] )\n",
    "            \n",
    "            annotated_vcf.write('\\n'.join(lines)+ '\\n')\n",
    "            annotated_vcf.write(line + '\\n')\n",
    "            \n",
    "        \n",
    "            header = lin_spl\n",
    "            header_dict = {l:i for l,i in zip(lin_spl, range(0, len(lin_spl)))}\n",
    "            \n",
    "            samples = header[9:]\n",
    "            info_cols = header[:9]\n",
    "            cols_maf = ['NNREF', 'NNREF_UUIDs','ALTAF', 'REFAF', 'MAF', 'Minor_Allele', 'NREF', 'UUIDS_REF', 'NMissing']\n",
    "            cols_extracted = ['SVTYPE','MSQ', 'SU', 'PE', 'SR', 'SVLEN', 'AF', 'NSAMP', 'CIPOS', 'CIEND', 'END'] \n",
    "            cols_maf_unr = [i+ '_unr' for i in cols_maf]\n",
    "            cols_maf_non_ipsc = [i+ '_non_ipsc' for i in cols_maf]\n",
    "            \n",
    "            \n",
    "            #remove the info column for space savings\n",
    "            \n",
    "            \n",
    "            info_header = ['NAME'] +info_cols[:-2] + cols_extracted + ['Missing_GTs', 'Missing_AB'] + cols_maf + cols_maf_unr + cols_maf_non_ipsc\n",
    "            \n",
    "#             info_header.remove('INFO')\n",
    "            \n",
    "            genotypes_header = ['NAME'] + samples\n",
    "            \n",
    "            gt_file.write(\"\\t\".join(genotypes_header) + '\\n')\n",
    "            gt_filt_file.write(\"\\t\".join(genotypes_header) + '\\n')\n",
    "            \n",
    "            ab_file.write(\"\\t\".join(genotypes_header) + '\\n')\n",
    "            ab_filt_file.write(\"\\t\".join(genotypes_header) + '\\n')\n",
    "            \n",
    "            info_file.write(\"\\t\".join(info_header) + '\\n')\n",
    "            info_filt_file.write(\"\\t\".join(info_header) + '\\n')\n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "            \n",
    "        elif count > header_end:\n",
    "            \n",
    "            if count == header_end+1:\n",
    "            \n",
    "                format_fields = lin_spl[header_dict['FORMAT']].split(':')\n",
    "                format_dict = {l:i for l,i in zip(format_fields, range(0, len(format_fields)))}\n",
    "                  \n",
    "            \n",
    "            sample_gts = []\n",
    "            sample_ab = []\n",
    "            # original first 9 columns\n",
    "            \n",
    "            info_col = header_dict['INFO']\n",
    "            \n",
    "            \n",
    "         \n",
    "            \n",
    "            POS = lin_spl[header_dict['POS']]\n",
    "            \n",
    "            ID = lin_spl[header_dict['ID']]\n",
    "            \n",
    "            info = lin_spl[header_dict['INFO']]\n",
    "            chrom = lin_spl[header_dict['#CHROM']]\n",
    "            \n",
    "      \n",
    "            \n",
    "            cols_to_parse = ['SVTYPE','MSQ', 'SU', 'PE', 'SR', 'SVLEN', 'AF', 'NSAMP', 'CIPOS', 'CIEND']\n",
    "            \n",
    "            cols_dict = dict(zip(cols_to_parse, range(0, len(cols_to_parse))))\n",
    "            \n",
    "            info_str = lin_spl[header_dict['INFO']]\n",
    "            \n",
    "            lin_spl[header_dict['INFO']] = info_str.replace('MEI', 'rMEI') \n",
    "            info = lin_spl[header_dict['INFO']]\n",
    "            \n",
    "            info_out = []\n",
    "            for l in cols_to_parse:\n",
    "                c = parse_info_col(lin_spl[header_dict['INFO']], l)\n",
    "                \n",
    "                info_out.append(c)\n",
    "            \n",
    "            \n",
    "            try:\n",
    "                MSQ = float(info_out[cols_dict['MSQ']])\n",
    "               \n",
    "            except:\n",
    "                MSQ = info_out[cols_dict['MSQ']]\n",
    "                \n",
    "            \n",
    "            \n",
    "            SVTYPE = info_out[cols_dict['SVTYPE']]\n",
    "            \n",
    "            # correct naming for downstream\n",
    "#             if SVTYPE == 'MEI':\n",
    "#                 info_out[cols_dict['SVTYPE']] = 'rMEI'\n",
    "#                 #correct the SVTYPE in vcf out_line- make it easier to combine all the vcfs if needed\n",
    "                \n",
    "#                 lin_spl[header_dict['INFO']] = lin_spl[header_dict['INFO']].replace('MEI', 'rMEI') \n",
    "#                 print l\n",
    "            \n",
    "            \n",
    "            SU = int(info_out[cols_dict['SU']])\n",
    "            SR = int(info_out[cols_dict['SR']])\n",
    "            PE = int(info_out[cols_dict['PE']])\n",
    "            QUAL = float(lin_spl[header_dict['QUAL']])\n",
    "            SVLEN = info_out[cols_dict['SVLEN']]\n",
    "            NSAMP = info_out[cols_dict['NSAMP']]\n",
    "            \n",
    "            \n",
    "   \n",
    "                \n",
    "          \n",
    "        \n",
    "            # give the variants more useful IDs \n",
    "            END = int(POS) + 1\n",
    "            \n",
    "            if SVTYPE != 'BND':\n",
    "                END = parse_info_col(info, 'END')\n",
    "                \n",
    "                ID_MOD = \"{}_{}_{}_{}\".format(SVTYPE, chrom, POS, END)\n",
    "            else:\n",
    "                # these need to be different, because they have no end, and will be non-unique\n",
    "            \n",
    "                ID_MOD = 'BND_{}_{}_{}'.format(chrom, POS, ID)\n",
    "            \n",
    "            lin_spl[header_dict['ID']] = ID_MOD\n",
    "            \n",
    "            # add minor allele frequency:\n",
    "            \n",
    "            gts_out = []\n",
    "            ab_out = []\n",
    "            \n",
    "            # all samples in this case\n",
    "            \n",
    "            for s in samples:\n",
    "                d = lin_spl[header_dict[s]].split(':')\n",
    "                gt = d[format_dict['GT']]\n",
    "                ab = d[format_dict['AB']]\n",
    "                gts_out.append(gt)\n",
    "                ab_out.append(ab)\n",
    "            \n",
    "            \n",
    "            missing_gts = ('./.' in gts_out)\n",
    "            \n",
    "            missing_ab = ('.' in ab_out)\n",
    "            \n",
    " \n",
    "            \n",
    "            # minor allele frequencies\n",
    "            maf_data_all = calculate_minor_allele_freq(gts_out, samples, chrom, sex_dict)\n",
    "        \n",
    "            samples_unr_gts = [gts_out[samples.index(i)] for i in unrelated_samples]\n",
    "            maf_data_unr= calculate_minor_allele_freq(samples_unr_gts, unrelated_samples, chrom, sex_dict)\n",
    "            \n",
    "            s = [gts_out[samples.index(i)] for i in non_ipsc_samples] \n",
    "            maf_data_non_ipsc = calculate_minor_allele_freq(s, non_ipsc_samples, chrom, sex_dict)\n",
    "            \n",
    "            # number of non ref samples in non-ipsc samples \n",
    "            non_ipsc_nnref = int(maf_data_non_ipsc[0])\n",
    "            num_missing_non_ipsc = int(maf_data_non_ipsc[8])\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            # Variant Filtering - Annotate the FILTER column with detailed filters\n",
    "            \n",
    "            FILTER = filter_variants(NSAMP, MSQ, QUAL, SVTYPE, PE, SR, SVLEN, SU, chrom,chroms, non_ipsc_nnref, num_missing_non_ipsc)\n",
    "            msq_filt = MSQ_filter_specific_classes(NSAMP, MSQ, QUAL, SVTYPE, PE, SR, SVLEN, SU, chrom,chroms, non_ipsc_nnref)\n",
    "            if msq_filt:\n",
    "                if FILTER != 'PASS':\n",
    "                    FILTER += \";MSQ\"\n",
    "                else:\n",
    "                    FILTER = 'MSQ'\n",
    "            \n",
    " \n",
    "            # replace the FILTER with my FILTER column\n",
    "            lin_spl[header_dict['FILTER']] = FILTER\n",
    "            \n",
    "            \n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "            # write out data\n",
    "            annotated_vcf.write(\"\\t\".join(lin_spl) + '\\n')\n",
    "            \n",
    "            \n",
    "            first_cols = lin_spl[:7]\n",
    "            \n",
    "            out_line_info =  [ID_MOD] + first_cols + info_out + [END] + [missing_gts] + [missing_ab] + maf_data_all + maf_data_unr + maf_data_non_ipsc\n",
    "            \n",
    "            out_line_info = map(str, out_line_info)\n",
    "            info_file.write(\"\\t\".join(out_line_info) + '\\n')\n",
    "            \n",
    "            \n",
    "            out_line_gt = [ID_MOD] + gts_out\n",
    "            gt_file.write(\"\\t\".join(out_line_gt) + '\\n')\n",
    "            \n",
    "            \n",
    "            out_line_ab = [ID_MOD] + ab_out\n",
    "            ab_file.write(\"\\t\".join(out_line_ab) + '\\n')\n",
    "            \n",
    "            \n",
    "            \n",
    "            # check specific filter tags\n",
    "            def check_filters(x):\n",
    "                filters = ['PE/SR', 'iPSC', 'ac0','OTHER_CONTIG', 'SHORT_LENGTH', 'SHORT_DEL']\n",
    "                for i in filters:\n",
    "                    if x.find(i) != -1:\n",
    "                        return True\n",
    "                return False\n",
    "            \n",
    "            \n",
    "            # check for specific flat filters (no MSQ)\n",
    "            if not check_filters(FILTER):\n",
    "                info_filt_file.write(\"\\t\".join(out_line_info) + '\\n')\n",
    "                gt_filt_file.write(\"\\t\".join(out_line_gt) + '\\n')\n",
    "                ab_filt_file.write(\"\\t\".join(out_line_ab) + '\\n')\n",
    "                \n",
    "            \n",
    "                \n",
    "            \n",
    "#             if FILTER == 'PASS':\n",
    "            \n",
    "#                 info_filt_file.write(\"\\t\".join(out_line_info) + '\\n')\n",
    "            \n",
    "#                 gt_filt_file.write(\"\\t\".join(out_line_gt) + '\\n')\n",
    "\n",
    "#                 ab_filt_file.write(\"\\t\".join(out_line_ab) + '\\n')\n",
    "                      \n",
    "        \n",
    "        count +=1\n",
    "        progress_level += 1  \n",
    "    \n",
    "    # close out files\n",
    "    F.close()\n",
    "    gt_file.close()\n",
    "    ab_file.close()\n",
    "    info_file.close()\n",
    "    annotated_vcf.close()\n",
    "    gt_filt_file.close()\n",
    "    ab_filt_file.close()\n",
    "    info_filt_file.close()\n",
    "    return fn_annotated_vcf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MSQ_filter_specific_classes(NSAMP, MSQ, QUAL, SVTYPE, PE, SR, SVLEN, SU, chrom, chroms, nnref_non_ipsc, msq_inv = 90, msq_mei = 20, msq_dup = 100, msq_del = 20, msq_bnd = 90):\n",
    "    \n",
    "    if type(MSQ) == float:\n",
    "        if SVTYPE == 'BND':\n",
    "            if MSQ < msq_bnd:\n",
    "                return True\n",
    "        elif SVTYPE == 'DUP':\n",
    "            if MSQ < msq_dup:\n",
    "                return True\n",
    "            \n",
    "        elif SVTYPE == 'DEL':\n",
    "            if MSQ < msq_del:\n",
    "                return True\n",
    "        \n",
    "        elif SVTYPE == 'rMEI':\n",
    "            if MSQ < msq_mei:\n",
    "                return True\n",
    "        \n",
    "        elif SVTYPE == 'INV':\n",
    "            if MSQ < msq_inv:\n",
    "                return True\n",
    "            \n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        # MSQ is missing\n",
    "        return True\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_variants(NSAMP, MSQ, QUAL, SVTYPE, PE, SR, SVLEN, SU, chrom, chroms, nnref_non_ipsc, num_missing_non_ipsc):\n",
    "        FILTER = []\n",
    "        \n",
    "        if int(NSAMP) == 0:\n",
    "            FILTER.append('ac0')\n",
    "        \n",
    "\n",
    "        if ((num_missing_non_ipsc/478) > 0.1):\n",
    "            FILTER.append('NMissing')\n",
    "\n",
    "    \n",
    "#         if SU < 8:\n",
    "#             FILTER.append('SU<8')\n",
    "            \n",
    "        if chrom not in chroms:\n",
    "            FILTER.append('OTHER_CONTIG')\n",
    "\n",
    "        if SVTYPE != 'BND':\n",
    "            SVLEN = abs(int(SVLEN))\n",
    "            if SVLEN < 50:\n",
    "                FILTER.append('SHORT_LENGTH')\n",
    "\n",
    "            if (SVTYPE == 'DEL') & (SVLEN < 418) & (int(SR) == 0):\n",
    "                FILTER.append('SHORT_DEL')\n",
    "            if SVTYPE == 'INV':\n",
    "                # require qual of 100 and ratio of PE/SU at least 10%\n",
    "                if (QUAL < 100) | (QUAL == '.'):\n",
    "                    FILTER.append('QUAL')\n",
    "\n",
    "                if filter_PE_SR_Ratio(PE, SR, recip_ratio=0.1):\n",
    "                    FILTER.append('PE/SR')\n",
    "\n",
    "\n",
    "\n",
    "        if SVTYPE == 'BND':\n",
    "            if QUAL == '.':\n",
    "                FILTER.append('QUAL')\n",
    "            # require qual of 100 and ratio of PE/SU at least 25%\n",
    "            if QUAL < 100:\n",
    "                FILTER.append('QUAL')\n",
    "\n",
    "            if filter_PE_SR_Ratio(PE, SR, recip_ratio=0.25):\n",
    "                FILTER.append('PE/SR')\n",
    "                \n",
    "        if nnref_non_ipsc == 0:\n",
    "            FILTER.append('iPSC')\n",
    "\n",
    "\n",
    "        if len(FILTER) == 0:\n",
    "            FILTER = ['PASS']\n",
    "\n",
    "        return \";\".join(FILTER)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_PE_SR_Ratio(PE, SR, recip_ratio = 0.25):\n",
    "    \"\"\" apply filtering similar to GTeX on these \n",
    "    variant types (making sure the required amount of evidence is present)\"\"\"\n",
    "\n",
    "    tot = PE + SR\n",
    "    for s in [PE,SR]:\n",
    "        frac = s/tot\n",
    "        if frac < recip_ratio:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def safe_div(x, y, alt=0):\n",
    "    try:\n",
    "        return x/y\n",
    "    except:\n",
    "        return alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_minor_allele_freq(gts, uuids, chrom, sex_dict):\n",
    "    \"\"\" calculate minor allele frequency, assume missing genotypes are reference\"\"\"\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    #uuids containing the 1 allele\n",
    "    non_ref = 0\n",
    "    non_ref_uuids = []\n",
    "\n",
    "    #uuids containing the 0 allele \n",
    "    ref = 0\n",
    "    ref_uuids = []\n",
    "\n",
    "    ref_allele = 0\n",
    "    alt_allele = 0\n",
    "\n",
    "    # for chroms that are diploid\n",
    "    # correct for allelic probs on sex chroms\n",
    "    gts_dict = {'0/0':0, '0/1':1, '1/1':2, './.':0}\n",
    "    gts_dict_sex = {'0/0':0, '0/1':1, '1/1':1, './.':0}\n",
    "    missing = 0\n",
    "\n",
    "    count = 0\n",
    "    for c in gts:\n",
    "        uuid = uuids[count]\n",
    "        sex = sex_dict[uuid]\n",
    "        \n",
    "        if c == './.':\n",
    "            missing +=1\n",
    "            # skip onward- we are ignoring missing\n",
    "            continue\n",
    "\n",
    "        if chrom=='Y':\n",
    "            if sex == 'M':\n",
    "                aa = gts_dict_sex[c]\n",
    "                ra = 1 - aa\n",
    "                \n",
    "            else:\n",
    "                count +=1\n",
    "                continue\n",
    "\n",
    "\n",
    "\n",
    "        elif chrom=='X':\n",
    "            if sex == 'M':\n",
    "                aa = gts_dict_sex[c]\n",
    "                ra = 1 - aa\n",
    "            else:\n",
    "                aa = gts_dict[c]\n",
    "                ra = 2 - aa\n",
    "\n",
    "\n",
    "        else:\n",
    "            aa = gts_dict[c]\n",
    "            ra = 2 - aa\n",
    "#                 ref_allele += ra\n",
    "#                 alt_allele += aa\n",
    "\n",
    "        ref_allele += ra\n",
    "        alt_allele += aa\n",
    "\n",
    "        if c not in ['0/0','./.']:\n",
    "            non_ref +=1\n",
    "            non_ref_uuids.append(uuids[count])\n",
    "\n",
    "        if c == '0/0':\n",
    "            ref +=1\n",
    "            ref_uuids.append(uuids[count])\n",
    "\n",
    "        count +=1\n",
    "\n",
    "\n",
    "\n",
    "    tot_alleles = ref_allele + alt_allele\n",
    "\n",
    "\n",
    "    ref_af = safe_div(ref_allele, tot_alleles, alt='All Missing')\n",
    "    alt_af = safe_div(alt_allele, tot_alleles, alt='All Missing')\n",
    "\n",
    "    afs = [ref_af, alt_af]\n",
    "    \n",
    "    \n",
    "    maf = 0.0\n",
    "    minor_allele = 'ALT'\n",
    "    if not tot_alleles == 0:\n",
    "        maf = min(afs)\n",
    "        if afs.index(maf) == 0:\n",
    "            minor_allele = 'REF'\n",
    "        else:\n",
    "            minor_allele = 'ALT'\n",
    "    \n",
    "    out_names = ['NNREF', 'NNREF_UUIDs','ALTAF', 'REFAF', 'MAF', 'Minor_Allele', 'NREF', 'UUIDS_REF', 'Missing']\n",
    "    out_data = [non_ref, \",\".join(non_ref_uuids), alt_af, ref_af, maf, minor_allele, ref, \",\".join(ref_uuids), missing]\n",
    "    \n",
    "    data_dict = dict(zip(out_names, out_data))\n",
    "    \n",
    "    \n",
    "    return out_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_arguments_to_parser(parser):\n",
    "    \n",
    "\n",
    "    parser.add_argument(\"-vcf\", \"--vcf\", dest=\"vcf_file\", metavar='<vcf_file>', help=\"vcf file from lumpy/speedseq pipeline, may be gzipped or not\", required=True)\n",
    "\n",
    "    \n",
    "    parser.add_argument(\"-gender\", \"--gender_map\", dest=\"gender_map\", metavar='<gender.txt>', help=\"gender_map_file\", required=True)\n",
    "        \n",
    "    \n",
    "    parser.add_argument(\"-unr\", \"--unrelated_samples\", dest=\"unr_samples\", metavar='<unrelated_samples.txt>', help=\"list of sample names that indicate unrelated samples\", required=True)\n",
    "        \n",
    "       \n",
    "    parser.add_argument(\"-non_ipsc\", \"--non_ipsc_samples\", dest=\"non_ipsc_samples\", metavar='<non_ipsc_samples.txt>', help=\"list of sample names that indicate samples that are not iPSC\", required=True)\n",
    "  \n",
    "    \n",
    "    parser.add_argument(\"-o\", \"--output_dir\", dest=\"output_dir\", metavar='<out_dir>', help=\"output directory for summary output\", required=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    parser.set_defaults(entry_point=run_from_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def command_parser():\n",
    "    parser = argparse.ArgumentParser(description= 'command line utility to annotate extracted info and genotypes from lumpy/melt with various things such as gene intersections, minor allele frequency etc.')\n",
    "    add_arguments_to_parser(parser)\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_from_args(args):\n",
    "        \n",
    "    unrelated_fn = args.unr_samples\n",
    "    non_ipsc_fn = args.non_ipsc_samples\n",
    "\n",
    "    unr_samples = [line.rstrip() for line in open(unrelated_fn)]\n",
    "    non_ipsc_samples = [line.rstrip() for line in open(non_ipsc_fn)]\n",
    "    \n",
    "    \n",
    "    gender_fn = args.gender_map\n",
    "    sex_dict = {line.rstrip().split()[0]:line.rstrip().split()[1] for line in open(gender_fn)}\n",
    "    d = datetime.datetime.now()\n",
    "    ts = d.strftime('%D- %H:%M')\n",
    "\n",
    "    print \"extracting variants, annotating filters and MAFs, start {}\".format(ts)\n",
    "    \n",
    "    fn_annotated_vcf = add_annotations_extract_gts(args.vcf_file, args.output_dir, unr_samples, non_ipsc_samples, sex_dict)\n",
    "    \n",
    "    d = datetime.datetime.now()\n",
    "    ts = d.strftime('%D- %H:%M')\n",
    "    \n",
    "    print 'complete', ts\n",
    "    print 'annotated vcf written'\n",
    "    print fn_annotated_vcf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = command_parser()\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    sys.exit(args.entry_point(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
