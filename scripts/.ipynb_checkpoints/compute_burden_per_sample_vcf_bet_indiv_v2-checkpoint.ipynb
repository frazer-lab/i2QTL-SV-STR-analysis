{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "from subprocess import call\n",
    "import subprocess\n",
    "import glob\n",
    "import djPyBio as DJ\n",
    "from djPyBio import Common as CM\n",
    "import pandas as pd\n",
    "import csv\n",
    "import copy \n",
    "import pybedtools as pbt\n",
    "import ciepy\n",
    "import cardipspy as cpy\n",
    "import itertools\n",
    "import tempfile\n",
    "import six\n",
    "import networkx as nx\n",
    "import scipy.stats as stats\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "import argparse\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "from mpl_toolkits.axes_grid1 import  make_axes_locatable\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_header_end(fn):\n",
    "    \"\"\" find header end line number of vcf or gzipped vcf\"\"\"\n",
    "\n",
    "    if fn.split('.').pop() == 'gz':\n",
    "        import gzip\n",
    "        F = gzip.open(fn, 'rU')\n",
    "    else:\n",
    "        F = open(fn, 'rU')\n",
    "\n",
    "\n",
    "    count = 0\n",
    "    for line in F:\n",
    "        count +=1\n",
    "        try: \n",
    "            spl = line.split('\\t')\n",
    "            spl0 = spl[0] \n",
    "            if spl[0]==\"#CHROM\":\n",
    "                F.close()\n",
    "                return count\n",
    "            if count > 2000:\n",
    "                F.close()\n",
    "                return 'incomplete or missing header, or very long header'\n",
    "        except:\n",
    "            break\n",
    "\n",
    "    F.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def parse_info_col(t, lab, type_out = str, alternative = 'None'):\n",
    "    t = t.split(';')\n",
    "    # filter out the tags with no keyword, if any\n",
    "    t = [i for i in t if i.find('=') != -1]\n",
    "    cols = [i.split('=')[0] for i in t]\n",
    "    try:\n",
    "        vals = [i.split('=')[1] for i in t]    \n",
    "    except:\n",
    "        return \"PARSE_ERROR\"\n",
    "    try:\n",
    "        ind = cols.index(lab)\n",
    "        v = vals[ind]\n",
    "        \n",
    "        try:\n",
    "            return type_out(v)\n",
    "        except:\n",
    "            return alternative\n",
    "    except:\n",
    "        return 'Column_Not_Present'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_format_col(t, col, lab, type_out = str, alternative = 'None', change_dict = False):\n",
    "    \"\"\" use the format column to pull out info of a specific VCF record\n",
    "    in a genotyping matrix (unparsed)\"\"\"\n",
    "    \n",
    "    f= t['FORMAT']\n",
    "    c = t[col]\n",
    "    c  = c.split(':')\n",
    "    f = f.split(':')\n",
    "    \n",
    "    try:\n",
    "        ind = f.index(lab)\n",
    "        v = c[ind]\n",
    "        try:\n",
    "            if change_dict:\n",
    "                try:\n",
    "                    v = change_dict.get(v, v)\n",
    "                except:\n",
    "                    return 'ERROR'\n",
    "            \n",
    "            return type_out(v)\n",
    "        except:\n",
    "            return alternative\n",
    "            \n",
    "    except:\n",
    "        return 'Column_Not_Present'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_format_mod(t, col, lab, except_out = 'None'):\n",
    "    \"\"\" use the format column to pull out info of a specific VCF record\n",
    "    in a genotyping matrix (unparsed)\"\"\"\n",
    "    \n",
    "    f= t['FORMAT']\n",
    "    c = t[col]\n",
    "    c  = c.split(':')\n",
    "    f = f.split(':')\n",
    "    \n",
    "    try:\n",
    "        ind = f.index(lab)\n",
    "        v = c[ind]\n",
    "        return v\n",
    "    except:\n",
    "        return except_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def coord_extract(df,chrom, start, end, contained = True):\n",
    "    \n",
    "    if contained:\n",
    "        return df[(df.Chr==chrom) & (df.POS >= start) & (df.END <= end)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_geno_fields(labs, samples, lin_spl, header_dict, format_dict):\n",
    "    gts_out = [{} for l in labs]\n",
    "    missing_samps = []\n",
    "    for s in samples:\n",
    "        d = lin_spl[header_dict[s]].split(':')\n",
    "        # cases where we don't have format info for all categories\n",
    "        if len(d) == 1:\n",
    "            if d[0] == '.':\n",
    "                missing_samps.append(s)\n",
    "        \n",
    "        for i,l in enumerate(labs):\n",
    "            if len(d) == 1:\n",
    "                if d[0] == '.':\n",
    "                    gt = './.'\n",
    "                \n",
    "            else:\n",
    "                gt = d[format_dict[l]]\n",
    "                if gt in ['.', './.']:\n",
    "                    gt = './.'\n",
    "                    missing_samps.append(s)\n",
    "                \n",
    "                \n",
    "            \n",
    "            gts_out[i][s] = gt\n",
    "    missing_samps = list(set(missing_samps))\n",
    "    return gts_out, missing_samps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def safe_div(x, y, alt=0):\n",
    "    try:\n",
    "        return x/y\n",
    "    except:\n",
    "        return alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_max_min_dict(d, samples):\n",
    "    v = [d[s] for s in samples]\n",
    "    v = [float(i) for i in v if i != './.']\n",
    "    max_v = round(max(v),4)\n",
    "    min_v = round(min(v), 4)\n",
    "    \n",
    "    return max_v, min_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_counts_to_per_sample(d, samples, cumulative_dict):\n",
    "    \n",
    "    for s in samples:\n",
    "        c = str(d[s])\n",
    "        if c== '0':\n",
    "            cat = 'REF'\n",
    "        \n",
    "        elif c in ['./.', '.']:\n",
    "            cat = 'MISSING'\n",
    "        \n",
    "        else:\n",
    "            cat = 'NREF'\n",
    "        \n",
    "        cumulative_dict[s][cat] = cumulative_dict[s].get(cat, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_counts_to_per_sample(d, samples, caller, cumulative_dict):\n",
    "    \n",
    "    for s in samples:\n",
    "        if caller in {'MELT', 'LUMPY'}: \n",
    "            c = str(d[s])\n",
    "            if c== '0':\n",
    "                cat = 'REF'\n",
    "\n",
    "            elif c in ['./.', '.']:\n",
    "                cat = 'MISSING'\n",
    "\n",
    "            else:\n",
    "                cat = 'NREF'\n",
    "\n",
    "            cumulative_dict[s][cat] = cumulative_dict[s].get(cat, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pair_concordance(pair_samples, data_dict):\n",
    "         \n",
    "    number_with_var = 0\n",
    "    number_concordant_with_var = 0\n",
    "    number_discordant_with_var = 0\n",
    "    number_missing = 0\n",
    "    per_pair_data = []\n",
    "    for tp in pair_samples:\n",
    "\n",
    "        vals =  map(str, [data_dict[tp[0]], data_dict[tp[1]]])\n",
    "\n",
    "        # check one way- first twin- for variant\n",
    "        if vals[0] == './.':\n",
    "            number_missing +=1\n",
    "            per_pair_data.append('MISSING')\n",
    "        \n",
    "        if vals[0] not in ['0|0', './.', '0']:\n",
    "\n",
    "            number_with_var +=1\n",
    "            if vals[0] == vals[1]:\n",
    "#                 tps_concordant.append(\"_\".join(tp))\n",
    "                number_concordant_with_var +=1\n",
    "                per_pair_data.append('CONCORDANT')\n",
    "            else:\n",
    "#                 tps_discordant.append(\"_\".join(tp))\n",
    "                number_discordant_with_var +=1 \n",
    "                per_pair_data.append('DISCORDANT')\n",
    "        elif vals[0] in ['0|0', '0']:\n",
    "            per_pair_data.append('REF')\n",
    "\n",
    "    if number_with_var > 0:\n",
    "        # replication rate- ranges from 0-100% 100% means every variant is a match\n",
    "        replication_rate = 1 - (number_discordant_with_var/number_with_var)\n",
    "    else:\n",
    "        replication_rate = 'None'\n",
    "    \n",
    "    if number_with_var > 0:\n",
    "        return [per_pair_data, number_with_var, number_concordant_with_var, number_discordant_with_var,\n",
    "                number_missing, replication_rate]\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_minor_allele_freq(gt_dict, uuids, chrom, sex_dict):\n",
    "    \"\"\" calculate minor allele frequency, assume missing genotypes are reference\"\"\"\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    #uuids containing the 1 allele\n",
    "    non_ref = 0\n",
    "    non_ref_uuids = []\n",
    "\n",
    "    #uuids containing the 0 allele \n",
    "    ref = 0\n",
    "    ref_uuids = []\n",
    "\n",
    "    ref_allele = 0\n",
    "    alt_allele = 0\n",
    "\n",
    "    # for chroms that are diploid\n",
    "    # correct for allelic probs on sex chroms\n",
    "    gts_dict = {'0/0':0, '0/1':1, '1/1':2, './.':0}\n",
    "    gts_dict_sex = {'0/0':0, '0/1':1, '1/1':1, './.':0}\n",
    "    missing = 0\n",
    "\n",
    "    count = 0\n",
    "    for uuid in uuids:\n",
    "        c = gt_dict[uuid]\n",
    "        sex = sex_dict[uuid]\n",
    "        \n",
    "        if c == './.':\n",
    "            missing +=1\n",
    "            # skip onward- we are ignoring missing\n",
    "            continue\n",
    "\n",
    "        if chrom=='Y':\n",
    "            if sex == 'M':\n",
    "                aa = gts_dict_sex[c]\n",
    "                ra = 1 - aa\n",
    "                \n",
    "            else:\n",
    "                count +=1\n",
    "                continue\n",
    "\n",
    "\n",
    "\n",
    "        elif chrom=='X':\n",
    "            if sex == 'M':\n",
    "                aa = gts_dict_sex[c]\n",
    "                ra = 1 - aa\n",
    "            else:\n",
    "                aa = gts_dict[c]\n",
    "                ra = 2 - aa\n",
    "\n",
    "\n",
    "        else:\n",
    "            aa = gts_dict[c]\n",
    "            ra = 2 - aa\n",
    "#                 ref_allele += ra\n",
    "#                 alt_allele += aa\n",
    "\n",
    "        ref_allele += ra\n",
    "        alt_allele += aa\n",
    "\n",
    "        if c not in ['0/0','./.']:\n",
    "            non_ref +=1\n",
    "            non_ref_uuids.append(uuids[count])\n",
    "\n",
    "        if c == '0/0':\n",
    "            ref +=1\n",
    "            ref_uuids.append(uuids[count])\n",
    "\n",
    "        count +=1\n",
    "\n",
    "\n",
    "\n",
    "    tot_alleles = ref_allele + alt_allele\n",
    "\n",
    "\n",
    "    ref_af = safe_div(ref_allele, tot_alleles, alt='All Missing')\n",
    "    alt_af = safe_div(alt_allele, tot_alleles, alt='All Missing')\n",
    "\n",
    "    afs = [ref_af, alt_af]\n",
    "    \n",
    "    \n",
    "    maf = 0.0\n",
    "    minor_allele = 'ALT'\n",
    "    if not tot_alleles == 0:\n",
    "        maf = min(afs)\n",
    "        if afs.index(maf) == 0:\n",
    "            minor_allele = 'REF'\n",
    "        else:\n",
    "            minor_allele = 'ALT'\n",
    "    \n",
    "    out_names = ['NREF', 'NREF_UUIDs','ALTAF', 'REFAF', 'MAF', 'Minor_Allele', 'REF', 'REF_UUIDS', 'Missing']\n",
    "    out_data = [non_ref, \",\".join(non_ref_uuids), alt_af, ref_af, maf, minor_allele, ref, \",\".join(ref_uuids), missing]\n",
    "    \n",
    "    data_dict = dict(zip(out_names, out_data))\n",
    "    \n",
    "    \n",
    "    return out_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_alt_allele_freq(gts_dict, samples, chrom):\n",
    "    \"\"\" calculate alt allele freq \"\"\"\n",
    "    \n",
    " \n",
    "    #uuids containing the 1 allele\n",
    "    non_ref = 0\n",
    "    non_ref_uuids = []\n",
    "\n",
    "    #uuids containing the 0 allele \n",
    "    ref = 0\n",
    "    ref_uuids = []\n",
    "    \n",
    "    ref_allele = 0\n",
    "    alt_allele = 0\n",
    "\n",
    "\n",
    "    # for chroms that are diploid\n",
    "    # correct for allelic probs on sex chroms\n",
    "\n",
    "    missing = 0\n",
    "    alleles_dist = {}\n",
    "    \n",
    "    allele_dict = {'0/0':0, '0/1':1, '1/1':2, './.':0}\n",
    "    allele_dict_sex = {'0/0':0, '0/1':1, '1/1':1, './.':0}\n",
    "    \n",
    "    # most common\n",
    "    s = set(samples)\n",
    "    gts_filt = {k:v for k,v in gts_dict.iteritems() if k in s}\n",
    "    vals = gts_filt.values()\n",
    "    C = Counter(vals)\n",
    "    mc = C.most_common()\n",
    "    try:\n",
    "        mc_allele, count_mc_allele = mc[0]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    for s in samples:\n",
    "        c = str(gts_dict[s])\n",
    "        \n",
    "        \n",
    "        alleles_dist[c] = alleles_dist.get(c, 0) + 1\n",
    "        aa = allele_dict[c]\n",
    "        ra = 2 - aa\n",
    "        \n",
    "        if c == './.':\n",
    "            missing +=1\n",
    "        \n",
    "        if c not in ['0|0', '0/0', './.', '0']:\n",
    "            non_ref +=1\n",
    "            non_ref_uuids.append(s)\n",
    "\n",
    "        if c in ['0|0', '0', '0/0']:\n",
    "            ref +=1\n",
    "            ref_uuids.append(s)\n",
    "        \n",
    "        \n",
    "        ref_allele += ra\n",
    "        alt_allele += aa\n",
    "        \n",
    "    \n",
    "    percent_missing = round((missing/len(samples)), 3)\n",
    "    \n",
    "    tot_alleles = ref_allele + alt_allele\n",
    "    ref_af = safe_div(ref_allele, tot_alleles, alt='All Missing')\n",
    "    alt_af = safe_div(alt_allele, tot_alleles, alt='All Missing')\n",
    "    afs = [ref_af, alt_af]\n",
    "    maf = 0.0\n",
    "    minor_allele = 'ALT'\n",
    "    if not tot_alleles == 0:\n",
    "        maf = min(afs)\n",
    "        if afs.index(maf) == 0:\n",
    "            minor_allele = 'REF'\n",
    "        else:\n",
    "            minor_allele = 'ALT'\n",
    "    \n",
    "    if (ref == 0) & (non_ref > 0): \n",
    "        NNREF_Freq = 1.0\n",
    "          \n",
    "    else:\n",
    "        NNREF_Freq = round(safe_div(non_ref, ref + non_ref, alt= 0), 4)\n",
    "        \n",
    "    \n",
    "    \n",
    "    major_allele = {'ALT':'REF', 'REF':'ALT'}[minor_allele]\n",
    "    out_names = ['NREF', 'REF', 'NMissing', 'NNREF_UUIDs','NNREF_AF', 'ALLELES_DIST', 'PERC_MISSING', 'MAF', \n",
    "                 'MINOR_ALLELE', 'MAJOR_ALLELE', 'MODE_ALLELE']\n",
    "    alleles_dist = \",\".join([\"{}:{}\".format(i,k) for i,k in alleles_dist.iteritems()])\n",
    "    out_data = [non_ref, ref, missing, \",\".join(non_ref_uuids), NNREF_Freq, alleles_dist, percent_missing, maf,\n",
    "                minor_allele, major_allele, mc_allele]\n",
    "    data_dict = dict(zip(out_names, out_data))\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prep_per_pair(df):\n",
    "    df = df.copy()\n",
    "    df = df.T\n",
    "    df['RR'] = df['CONCORDANT'] / (df['CONCORDANT'] + df['DISCORDANT'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prep_per_sample(df):\n",
    "    df = df.copy()\n",
    "    df = df.T\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MinMaxScaler(features, minimum = 0, maximum = 2):\n",
    "    min_features = min(features)\n",
    "    max_features = max(features)\n",
    "    \n",
    "    if max_features - min_features == 0:\n",
    "        return\n",
    "    \n",
    "    def transform(x, min_features, max_features, min_target, max_target):\n",
    "                \n",
    "        xstd = (x - min_features) / (max_features - min_features)\n",
    "        xscaled = xstd * (max_target - min_target) + min_target\n",
    "        \n",
    "        return xscaled\n",
    "        \n",
    "    features_out = [transform(i, min_features, max_features, minimum, maximum) for i in features]\n",
    "    return features_out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_rank_dict(ds_dict, sample_subset, type_out = int):\n",
    "    \"\"\"adjusted dosages - rank the dosages and put them on a 0-2 scale\"\"\"\n",
    "    \n",
    "    values = []\n",
    "    for k,v in ds_dict.iteritems():\n",
    "        \n",
    "        if k in sample_subset:\n",
    "            if v in ['./.', '.']:\n",
    "                pass\n",
    "            else:\n",
    "                values.append(type_out(v))\n",
    "\n",
    "    rank_vals = stats.rankdata(values)\n",
    "    \n",
    "    scale_ranks = MinMaxScaler(rank_vals)\n",
    "    scale_ranks = [round(i, 4) for i in scale_ranks]\n",
    "    rank_dict = dict(zip(map(str, values), scale_ranks))\n",
    "    return rank_dict\n",
    "\n",
    "def get_adjusted_ds_dict(ds_dict, rank_dict, type_in = int):\n",
    "    \n",
    "    adj_ds_dict = {}\n",
    "    \n",
    "    for k,v in ds_dict.iteritems():\n",
    "        try:\n",
    "            key_out = str(type_in(v))\n",
    "        except:\n",
    "            key_out = v\n",
    "            \n",
    "        rank_convert = rank_dict.get(key_out, -1)\n",
    "        adj_ds_dict[k] = rank_convert\n",
    "        \n",
    "    return adj_ds_dict\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ds_dict_biallelic_genotype(gt_dict, svtype):\n",
    "    \"\"\" compute the dosage from biallelic genotypes \"\"\"\n",
    "    \n",
    "    convert_dict_dup = {'0/0': 0, '0/1':1,  '1/1': 2}\n",
    "    convert_dict_del = {'0/0': 2, '0/1':1,  '1/1': 0}\n",
    "    ds_dict = {}\n",
    "    if svtype == 'DUP':\n",
    "        for k,v in gt_dict.iteritems():\n",
    "            ds_dict[k] = convert_dict_dup.get(v, -1)\n",
    "    \n",
    "    elif svtype in ['DEL', 'rMEI']:\n",
    "        for k,v in gt_dict.iteritems():\n",
    "            ds_dict[k] = convert_dict_del.get(v, -1)\n",
    "            \n",
    "    else:\n",
    "        # MOBILE ELEMENTS\n",
    "        for k,v in gt_dict.iteritems():\n",
    "            ds_dict[k] = convert_dict_dup.get(v, -1)\n",
    "            \n",
    "    return ds_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_maf_dosage(ds_dict, samples):\n",
    "    s = set(samples)\n",
    "    ds_dict_sub = {k:v for k,v in ds_dict.iteritems() if k in s}\n",
    "    vals = ds_dict_sub.values()\n",
    "    ignore = {'./.', '.'}\n",
    "    vals_corrected = [i for i in vals if i not in ignore]\n",
    "    C = Counter(vals_corrected)\n",
    "    mc = C.most_common()\n",
    "    try:\n",
    "        mc_allele, count_mc_allele = mc[0]\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    if len(mc) == 0:\n",
    "            num_passing = 0\n",
    "            num_alt = 0\n",
    "            mc_allele = './.'\n",
    "            count_mc_allele = len(ds_dict_sub)\n",
    "\n",
    "    else:\n",
    "        num_passing = len(vals_corrected)\n",
    "        num_alt = num_passing - count_mc_allele\n",
    "    \n",
    "    if num_alt > 0:\n",
    "        maf = num_alt/num_passing\n",
    "    \n",
    "    else:\n",
    "        maf = 0\n",
    "    \n",
    "    alleles_dist = {} \n",
    "    for s in samples:\n",
    "        c = str(ds_dict[s])\n",
    "        alleles_dist[c] = alleles_dist.get(c, 0) + 1\n",
    "    \n",
    "    alleles_dist = \",\".join([\"{}:{}\".format(i,k) for i,k in alleles_dist.iteritems()])\n",
    "    \n",
    "    return maf, count_mc_allele, mc_allele, num_passing, num_alt, alleles_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_maf_dosage_gs_ft(ds_dict,ft_dict, samples):\n",
    "    s = set(samples)\n",
    "    ds_dict_sub = {k:v for k,v in ds_dict.iteritems() if k in s}\n",
    "    ft_dict_sub =  {k:v for k,v in ft_dict.iteritems() if k in s}\n",
    "    lq = {k:v for k,v in ft_dict.iteritems() if v == 'LQ'}\n",
    "    \n",
    "    # add lq in place of genotypes\n",
    "    ds_dict_sub.update(lq)\n",
    "    # ignore them if LQ\n",
    "    vals = ds_dict_sub.values()\n",
    "    ignore = {'./.', '.', 'LQ'}\n",
    "    vals_corrected = [i for i in vals if i not in ignore]\n",
    "    C = Counter(vals_corrected)\n",
    "    mc = C.most_common()\n",
    "    try:\n",
    "        mc_allele, count_mc_allele = mc[0]\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    if len(mc) == 0:\n",
    "            num_passing = 0\n",
    "            num_alt = 0\n",
    "            mc_allele = 'LQ'\n",
    "            count_mc_allele = len(ds_dict_sub)\n",
    "\n",
    "    else:\n",
    "        num_passing = len(vals_corrected)\n",
    "        num_alt = num_passing - count_mc_allele\n",
    "    \n",
    "    if num_alt > 0:\n",
    "        maf = num_alt/num_passing\n",
    "    \n",
    "    else:\n",
    "        maf = 0\n",
    "    \n",
    "    alleles_dist = {} \n",
    "    for s in samples:\n",
    "        c = str(ds_dict_sub[s])\n",
    "        alleles_dist[c] = alleles_dist.get(c, 0) + 1\n",
    "    \n",
    "    alleles_dist = \",\".join([\"{}:{}\".format(i,k) for i,k in alleles_dist.iteritems()])\n",
    "    return maf, count_mc_allele, mc_allele, num_passing, num_alt, alleles_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dosage_bet_sample_pairs(d, sample_pairs, cumulative_dict, num_calls_dict, \n",
    "                                caller, svtype, svlen = False):\n",
    "    \"\"\"dosage difference between the compared samples - simplified- removing deletion/duplication for a specific memeber of a pair- just base pairs different due to this svtype\"\"\"\n",
    "    copies = {'DUP': {'0/0':0, '0/1': 1, '1/1':2}, 'DEL': {'0/0':2, '0/1': 1, '1/1':0}}\n",
    "    missing =  set(['./.', '.'])\n",
    "    for s1, s2 in sample_pairs:\n",
    "        # name of sample pair in dict\n",
    "        s = \"{}_{}\".format(s1,s2)\n",
    "        g = [d[s1], d[s2]]\n",
    "        c1, c2 = map(str, g)\n",
    "        sgt = set([c1,c2])\n",
    "        if (c1 == c2) | (len(sgt.intersection(missing)) > 0):\n",
    "            # if they are the same or one of the ones in pair is \"missing\" we will skip the site\n",
    "            continue\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            if caller == 'HipSTR':\n",
    "\n",
    "                c1, c2 = map(int, g)\n",
    "                \n",
    "                # difference between the pairs\n",
    "                c = c1 - c2 \n",
    "\n",
    "                if c != 0:\n",
    "                    cumulative_dict[caller][s][svtype] = cumulative_dict[caller][s][svtype] + c\n",
    "                    num_calls_dict[caller][s][svtype] = num_calls_dict[caller][s][svtype] + 1\n",
    "               \n",
    "            if caller in {'GS', 'GS_LCNV'}:\n",
    "                c1, c2 = map(int, g)\n",
    "                \n",
    "                diff = c1 - c2\n",
    "                \n",
    "                if svlen:\n",
    "                    bp = abs(diff) * svlen\n",
    "                    \n",
    "                    if diff != 0:\n",
    "                        cumulative_dict[caller][s][svtype] = cumulative_dict[caller][s][svtype] + bp\n",
    "                        num_calls_dict[caller][s][svtype] = num_calls_dict[caller][s][svtype] + 1\n",
    "               \n",
    "            if caller in {'MELT', 'LUMPY'}:  \n",
    "                \n",
    "                if (caller == 'MELT') | (svtype == 'DUP'):\n",
    "                    c1, c2 = copies['DUP'][c1], copies['DUP'][c2]\n",
    "                    \n",
    "                else:\n",
    "                    c1, c2 = copies['DEL'][c1], copies['DEL'][c2]\n",
    "                    \n",
    "                diff = c1 - c2 \n",
    "                bp = abs(diff) * abs(svlen)\n",
    "                if diff != 0:\n",
    "                    cumulative_dict[caller][s][svtype] = cumulative_dict[caller][s][svtype] + bp\n",
    "                    num_calls_dict[caller][s][svtype] = num_calls_dict[caller][s][svtype] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dosage_to_per_sample(d, samples, cumulative_dict, num_calls_dict, caller, svtype, svlen = False):\n",
    "    \n",
    "    copies = {'0/0':0, '0/1': 1, '1/1':2}\n",
    "    for s in samples:\n",
    "        c = str(d[s])\n",
    "        if c in {'./.', '.', '0/0'}:\n",
    "            continue\n",
    "        else:\n",
    "            \n",
    "            if caller == 'HipSTR':\n",
    "\n",
    "                c = int(c)\n",
    "                if c > 0:\n",
    "                    cumulative_dict[caller][s]['INS'] = cumulative_dict[caller][s]['INS'] + c\n",
    "                    num_calls_dict[caller][s]['INS'] = num_calls_dict[caller][s]['INS'] + 1\n",
    "                elif c < 0:\n",
    "                    cumulative_dict[caller][s]['DEL'] = cumulative_dict[caller][s]['DEL'] + abs(c)\n",
    "                    num_calls_dict[caller][s]['DEL'] = num_calls_dict[caller][s]['DEL'] + 1\n",
    "                else:\n",
    "                    pass\n",
    "            if caller in {'GS', 'GS_LCNV'}:\n",
    "                c = int(c)\n",
    "                if c == 2:\n",
    "                    pass\n",
    "                else:\n",
    "                    diff = c-2\n",
    "                    diff = abs(diff)\n",
    "                    if svlen:\n",
    "                        bp = diff * svlen\n",
    "                        if c > 2:\n",
    "                            cumulative_dict[caller][s]['DUP'] = cumulative_dict[caller][s]['DUP'] + bp\n",
    "                            num_calls_dict[caller][s]['DUP'] = num_calls_dict[caller][s]['DUP'] + 1\n",
    "\n",
    "                        elif c < 2:\n",
    "                            cumulative_dict[caller][s]['DEL'] = cumulative_dict[caller][s]['DEL'] + bp\n",
    "                            num_calls_dict[caller][s]['DEL'] = num_calls_dict[caller][s]['DEL'] + 1\n",
    "                            \n",
    "            if caller in {'MELT', 'LUMPY'}:                \n",
    "                diff = copies[c]\n",
    "                if svlen:\n",
    "                    bp = diff * abs(svlen)\n",
    "                    cumulative_dict[caller][s][svtype] = cumulative_dict[caller][s][svtype] + bp\n",
    "                    num_calls_dict[caller][s][svtype] = num_calls_dict[caller][s][svtype] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_per_sample_stats(d):\n",
    "    dfs = []\n",
    "    for k in d.keys():\n",
    "        df = pd.DataFrame(d[k])\n",
    "        df = df.T\n",
    "        df['CALLER'] = k\n",
    "        dfs.append(df)\n",
    "    out = pd.concat(dfs)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_vcf_and_annotate(fn, out_dir, sample_pairs, sample_subset, fn_out, include = False):\n",
    "    \n",
    "    header_end = find_header_end(fn)\n",
    "    \n",
    "    count = 0\n",
    "    progress_level = 0\n",
    "    progress_increment = 50000\n",
    "    \n",
    "    sp_names = ['_'.join(s) for s in sample_pairs]\n",
    "    if fn.split('.').pop() == 'gz':\n",
    "        F = gzip.open(fn, 'rU')\n",
    "    else:\n",
    "        F = open(fn, 'rU')\n",
    "    \n",
    "    fn_out_per_sample = os.path.join(out_dir, fn_out)\n",
    "    fn_out_count_per_sample = fn_out_per_sample.replace('.tsv', '.nnref.tsv')\n",
    "    \n",
    "    chroms = [str(i) for i in range(1,23)] + ['X', 'Y']\n",
    "    \n",
    "    \n",
    "#     IDs_set = set(IDs)\n",
    "\n",
    "    if include:\n",
    "        include = set([i.rstrip() for i in open(include, 'r')])\n",
    "    \n",
    "    count_missing = 0\n",
    "    num_variants = 0\n",
    "    num_variants_out = 0\n",
    "    d = datetime.datetime.now()\n",
    "    ts = d.strftime('%D- %H:%M')\n",
    "    print \"Starting Variant Processing: {}\".format(ts)\n",
    "    \n",
    "#     info_fields = 'INFRAME_PGEOM INFRAME_UP INFRAME_DOWN OUTFRAME_PGEOM OUTFRAME_UP OUTFRAME_DOWN BPDIFFS START END AN REFAC AC NSKIP NFILT DP DSNP DSTUTTER DFLANKINDEL'.split()\n",
    "            \n",
    "    add_id = False\n",
    "    add_info = False\n",
    "    count_num_length = 0\n",
    "    \n",
    "    \n",
    "    for line in F:\n",
    "        \n",
    "        if progress_level == progress_increment:\n",
    "            d = datetime.datetime.now()\n",
    "            ts = d.strftime('%D- %H:%M')\n",
    "            print \"processed {} variants {}\".format(count, ts)\n",
    "            progress_level = 0\n",
    "            \n",
    "        progress_level +=1\n",
    "\n",
    "\n",
    "        line = line.rstrip()\n",
    "        lin_spl = line.split('\\t')\n",
    "\n",
    "        if count < header_end-1:\n",
    "            count +=1\n",
    "            continue\n",
    "            \n",
    "        if count == header_end-1:\n",
    "\n",
    "            \n",
    "            header = copy.deepcopy(lin_spl)\n",
    "            header_dict = {l:i for l,i in zip(lin_spl, range(0, len(lin_spl)))}\n",
    "            info_head = header[:9]\n",
    "            samples = header[9:]\n",
    "            info_cols = header[:9]\n",
    "#             sample_order_dict = {s:header.index(s) for s in samples}\n",
    "            per_sample_burden  = {'HipSTR': {s:{'STR':0} for s in sp_names}}\n",
    "            d2  = {'GS': {s:{'DEL':0, 'DUP':0, 'mCNV':0} for s in sp_names}}\n",
    "            d3  = {'LUMPY': {s:{'DEL':0, 'DUP':0, 'rMEI':0} for s in sp_names}}\n",
    "            d4 = {'GS_LCNV': {s:{'DEL':0, 'DUP':0, 'mCNV':0} for s in sp_names}}\n",
    "            d5 = {'MELT': {s:{'LINE1':0, 'SVA':0,\n",
    "                              'ALU':0} for s in sp_names}}\n",
    "            \n",
    "            per_sample_burden.update(d2)\n",
    "            per_sample_burden.update(d3)\n",
    "            per_sample_burden.update(d4)\n",
    "            per_sample_burden.update(d5)\n",
    "            \n",
    "            per_sample_num_vars = copy.deepcopy(per_sample_burden)\n",
    "#             try:\n",
    "#                 reodered_samples = [header[sample_order_dict[i]] for i in sample_subset]\n",
    "#             except:\n",
    "#                 print sample_order_dict\n",
    "#                 print sample_subset\n",
    "#                 print samples\n",
    "#                 break\n",
    "\n",
    "            \n",
    "            count +=1\n",
    "            continue\n",
    "\n",
    "            \n",
    "\n",
    "        elif count > header_end:\n",
    "\n",
    "           \n",
    "\n",
    "            format_fields = lin_spl[header_dict['FORMAT']].split(':')\n",
    "            format_dict = {l:i for l,i in zip(format_fields, range(0, len(format_fields)))}\n",
    "\n",
    "            POS = lin_spl[header_dict['POS']]\n",
    "            ID = lin_spl[header_dict['ID']]\n",
    "            INFO = lin_spl[header_dict['INFO']]\n",
    "            chrom = str(lin_spl[header_dict['#CHROM']])\n",
    "            FILTER = str(lin_spl[header_dict['FILTER']])\n",
    "            \n",
    "            if include:\n",
    "                if ID in include:\n",
    "                    cols_info_extracted = ['SVTYPE', 'CALLER', 'SVLEN', 'END']\n",
    "                    cols_dict = dict(zip(cols_info_extracted, range(0, len(cols_info_extracted))))\n",
    "                    info_out = []\n",
    "                    info_cols_dict = {}\n",
    "                    for l in cols_info_extracted:\n",
    "                        c = parse_info_col(INFO, l)\n",
    "                        info_out.append(c)\n",
    "                        info_cols_dict[l] = c\n",
    "                    if info_cols_dict['CALLER'] == 'HipSTR':\n",
    "                        info_cols_dict['SVTYPE'] = 'STR'\n",
    "\n",
    "                    caller = info_cols_dict['CALLER']\n",
    "                    svtype = info_cols_dict['SVTYPE']\n",
    "                    svlen = info_cols_dict['SVLEN']\n",
    "                    END = info_cols_dict['END']\n",
    "                    \n",
    "                    if caller == 'LUMPY':\n",
    "                        if svtype in {'DUP', 'DEL', 'rMEI'}:\n",
    "                            fields_desired = ['GT']\n",
    "                            geno_fields_dict = {fields_desired[i]:i for i in range(0, len(fields_desired))}\n",
    "                            geno_extracted, missing_samps_gt = get_geno_fields(fields_desired, samples, lin_spl, \n",
    "                                                                        header_dict, format_dict)\n",
    "                            gt_dict = geno_extracted[geno_fields_dict['GT']]\n",
    "                            \n",
    "                            svlen = int(svlen)\n",
    "                            \n",
    "                            add_dosage_bet_sample_pairs(gt_dict, sample_pairs, \n",
    "                                                        per_sample_burden, per_sample_num_vars,\n",
    "                                                     caller, svtype, svlen = svlen)\n",
    "\n",
    "#                             add_dosage_to_per_sample(gt_dict, samples, per_sample_burden, per_sample_num_vars,\n",
    "#                                                      caller, svtype, svlen = svlen)\n",
    "\n",
    "                    if caller in ['GS', 'GS_LCNV']:\n",
    "                        svlen = int(info_cols_dict['END']) - int(POS)\n",
    "\n",
    "                        try:\n",
    "                            if caller == 'GS_LCNV':\n",
    "                                fields_desired = ['CN']\n",
    "                                geno_fields_dict = {fields_desired[i]:i for i in range(0, len(fields_desired))}\n",
    "                                geno_extracted, missing_samps = get_geno_fields(fields_desired, samples, lin_spl, \n",
    "                                                                        header_dict, format_dict)\n",
    "                            else:\n",
    "                                fields_desired = ['CN', 'FT']\n",
    "                                geno_fields_dict = {fields_desired[i]:i for i in range(0, len(fields_desired))}\n",
    "                                geno_extracted, missing_samps = get_geno_fields(fields_desired, samples, lin_spl, \n",
    "                                                                        header_dict, format_dict)\n",
    "                        except:\n",
    "                            print ID, format_fields, \n",
    "                            break\n",
    "                        \n",
    "                        svlen = int(svlen)\n",
    "                        cn_dict = geno_extracted[geno_fields_dict['CN']]\n",
    "                        \n",
    "                        add_dosage_bet_sample_pairs(cn_dict, sample_pairs, \n",
    "                                                    per_sample_burden, per_sample_num_vars, \n",
    "                                                 caller, svtype, svlen = svlen)\n",
    "#                         add_dosage_to_per_sample(cn_dict, samples, per_sample_burden, per_sample_num_vars, \n",
    "#                                                  caller, svtype, svlen = svlen)\n",
    "#                         maf, count_mc_allele, mc_allele, num_passing, num_alt, alleles_dist = compute_maf_dosage(cn_dict, sample_subset)\n",
    "\n",
    "                    if caller == 'MELT':\n",
    "                        fields_desired = ['GT']\n",
    "                        geno_fields_dict = {fields_desired[i]:i for i in range(0, len(fields_desired))}\n",
    "                        geno_extracted, missing_samps = get_geno_fields(fields_desired, samples, lin_spl, \n",
    "                                                                    header_dict, format_dict)\n",
    "\n",
    "                        gt_dict = geno_extracted[geno_fields_dict['GT']]  \n",
    "                        # some MELT variants aren't confident enough to have an SVLEN estimate, skip them\n",
    "                        svlen = int(svlen)\n",
    "                        if svlen > 0:\n",
    "                            add_dosage_bet_sample_pairs(gt_dict, sample_pairs, \n",
    "                                                        per_sample_burden, per_sample_num_vars, \n",
    "                                                     caller, \n",
    "                                                     svtype, svlen = svlen)\n",
    "#                             add_dosage_to_per_sample(gt_dict, samples, per_sample_burden, per_sample_num_vars, \n",
    "#                                                      caller, \n",
    "#                                                      svtype, svlen = svlen)\n",
    "\n",
    "                    if caller == 'HipSTR':\n",
    "                        svtype = 'STR'\n",
    "\n",
    "                        fields_desired = ['BPS']\n",
    "                        geno_fields_dict = {fields_desired[i]:i for i in range(0, len(fields_desired))}\n",
    "                        geno_extracted, missing_samps = get_geno_fields(fields_desired, samples, lin_spl, \n",
    "                                                                        header_dict, format_dict)\n",
    "                        ds_dict = geno_extracted[geno_fields_dict['BPS']]\n",
    "                        add_dosage_bet_sample_pairs(ds_dict, sample_pairs, \n",
    "                                                    per_sample_burden, \n",
    "                                                    per_sample_num_vars, caller, \n",
    "                                                     svtype, svlen = False)\n",
    "#                         add_dosage_to_per_sample(ds_dict, samples, per_sample_burden, per_sample_num_vars, caller, \n",
    "#                                                      svtype, svlen = False)\n",
    "#                         try:\n",
    "# #                             maf, count_mc_allele, mc_allele, num_passing, num_alt, alleles_dist = compute_maf_dosage(ds_dict, sample_subset)\n",
    "#                         except:\n",
    "#                             break\n",
    "\n",
    "                    num_variants_out +=1\n",
    "\n",
    "            num_variants +=1\n",
    "                      \n",
    "        count +=1\n",
    "        \n",
    "    per_sample_stats = aggregate_per_sample_stats(per_sample_burden)\n",
    "    per_sample_nnref = aggregate_per_sample_stats(per_sample_num_vars)\n",
    "    per_sample_stats.to_csv(fn_out_per_sample, sep = '\\t')\n",
    "    per_sample_nnref.to_csv(fn_out_count_per_sample, sep = '\\t')\n",
    "    \n",
    "    print \"number of variants processed: {}\".format(num_variants)\n",
    "    print \"number of variants outputted: {}\".format(num_variants_out)\n",
    "    d = datetime.datetime.now()\n",
    "    ts = d.strftime('%D- %H:%M')\n",
    "    print \"Completed Variant Processing: completed burden estimation {}\".format(ts)\n",
    "    print 'COMPLETE'\n",
    "    print 'files generated: {}'.format(fn_out_per_sample)\n",
    "    print 'files generated: {}'.format(fn_out_count_per_sample)\n",
    "    return\n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_arguments_to_parser(parser):\n",
    "    \n",
    "\n",
    "    parser.add_argument(\"-vcf\", \"--vcf\", dest=\"vcf_file\", metavar='<vcf_file>', help=\"vcf file from lumpy/speedseq pipeline, may be gzipped or not\", required=True)\n",
    "    \n",
    "    parser.add_argument(\"-s\", \"--samples\", dest=\"samples_fn\", metavar='<samples_fn>', help=\"file with samples to use to compute the maf and converted dosages\", required=False)\n",
    "    \n",
    "    parser.add_argument(\"-s_pairs\", \"--sample_pairs\", dest=\"sample_pairs_fn\", metavar='<samples_fn>', help=\"file with samples pairs to compare and track dosage differences between\", required=False)\n",
    "    \n",
    "    \n",
    "    parser.add_argument(\"-id_include\", \"--id_fn\", dest=\"id_fn\", metavar='<IDs to include in the final output>', help=\"file of pairs of IDs one per line to keep in the output\", required=False, default = False)\n",
    "    \n",
    "    parser.add_argument(\"-fn_out\", \"--fn_out\", dest=\"fn_out\", metavar='<file name of output file (NOT dir)>', help=\"file name of the vcf file- excluding the directory, just file name\", default = False, required=False)\n",
    "    \n",
    "    \n",
    "    parser.add_argument(\"-o\", \"--output_dir\", dest=\"output_dir\", metavar='<out_dir>', help=\"output directory for summary output\", required=False, default = '')\n",
    "    \n",
    "    parser.set_defaults(entry_point=run_from_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def command_parser():\n",
    "    parser = argparse.ArgumentParser(description= 'command line utility to extract info from full vcf including variants from lumpy, genome strip, melt and hipstr')\n",
    "    add_arguments_to_parser(parser)\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_from_args(args):\n",
    "    vcf_fn = args.vcf_file\n",
    "    id_fn = args.id_fn\n",
    "    samples_fn = args.samples_fn\n",
    "    fn_out = args.fn_out\n",
    "    out_dir = args.output_dir\n",
    "    sp_fn = args.sample_pairs_fn\n",
    "    \n",
    "    sample_pairs = [line.rstrip().split() for line in open(sp_fn)]\n",
    "    samples = [line.rstrip() for line in open(samples_fn)]\n",
    "    process_vcf_and_annotate(vcf_fn, out_dir, sample_pairs, samples, fn_out, include = id_fn)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = command_parser()\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    sys.exit(args.entry_point(args))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
