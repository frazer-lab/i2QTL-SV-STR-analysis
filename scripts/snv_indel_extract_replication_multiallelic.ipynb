{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "from subprocess import call\n",
    "import subprocess\n",
    "import glob\n",
    "import djPyBio as DJ\n",
    "from djPyBio import Common as CM\n",
    "import pandas as pd\n",
    "import csv\n",
    "import copy \n",
    "import pybedtools as pbt\n",
    "import ciepy\n",
    "import cardipspy as cpy\n",
    "import itertools\n",
    "import tempfile\n",
    "import six\n",
    "import networkx as nx\n",
    "import scipy.stats as stats\n",
    "\n",
    "import argparse\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "from mpl_toolkits.axes_grid1 import  make_axes_locatable\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_header_end(fn):\n",
    "    \"\"\" find header end line number of vcf or gzipped vcf\"\"\"\n",
    "\n",
    "    if fn.split('.').pop() == 'gz':\n",
    "        import gzip\n",
    "        F = gzip.open(fn, 'rU')\n",
    "    else:\n",
    "        F = open(fn, 'rU')\n",
    "\n",
    "\n",
    "    count = 0\n",
    "    for line in F:\n",
    "        count +=1\n",
    "        try: \n",
    "            spl = line.split('\\t')\n",
    "            spl0 = spl[0] \n",
    "            if spl[0]==\"#CHROM\":\n",
    "                F.close()\n",
    "                return count\n",
    "            if count > 2000:\n",
    "                F.close()\n",
    "                return 'incomplete or missing header, or very long header'\n",
    "        except:\n",
    "            break\n",
    "\n",
    "    F.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def parse_info_col(t, lab, type_out = str, alternative = 'None'):\n",
    "    t = t.split(';')\n",
    "    # filter out the tags with no keyword, if any\n",
    "    t = [i for i in t if i.find('=') != -1]\n",
    "    cols = [i.split('=')[0] for i in t]\n",
    "    try:\n",
    "        vals = [i.split('=')[1] for i in t]    \n",
    "    except:\n",
    "        return \"PARSE_ERROR\"\n",
    "    try:\n",
    "        ind = cols.index(lab)\n",
    "        v = vals[ind]\n",
    "        \n",
    "        try:\n",
    "            return type_out(v)\n",
    "        except:\n",
    "            return alternative\n",
    "    except:\n",
    "        return 'Column_Not_Present'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_format_col(t, col, lab, type_out = str, alternative = 'None', change_dict = False):\n",
    "    \"\"\" use the format column to pull out info of a specific VCF record\n",
    "    in a genotyping matrix (unparsed)\"\"\"\n",
    "    \n",
    "    f= t['FORMAT']\n",
    "    c = t[col]\n",
    "    c  = c.split(':')\n",
    "    f = f.split(':')\n",
    "    \n",
    "    try:\n",
    "        ind = f.index(lab)\n",
    "        v = c[ind]\n",
    "        try:\n",
    "            if change_dict:\n",
    "                try:\n",
    "                    v = change_dict.get(v, v)\n",
    "                except:\n",
    "                    return 'ERROR'\n",
    "            \n",
    "            return type_out(v)\n",
    "        except:\n",
    "            return alternative\n",
    "            \n",
    "    except:\n",
    "        return 'Column_Not_Present'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_format_mod(t, col, lab, except_out = 'None'):\n",
    "    \"\"\" use the format column to pull out info of a specific VCF record\n",
    "    in a genotyping matrix (unparsed)\"\"\"\n",
    "    \n",
    "    f= t['FORMAT']\n",
    "    c = t[col]\n",
    "    c  = c.split(':')\n",
    "    f = f.split(':')\n",
    "    \n",
    "    try:\n",
    "        ind = f.index(lab)\n",
    "        v = c[ind]\n",
    "        return v\n",
    "    except:\n",
    "        return except_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def coord_extract(df,chrom, start, end, contained = True):\n",
    "    \n",
    "    if contained:\n",
    "        return df[(df.Chr==chrom) & (df.POS >= start) & (df.END <= end)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_geno_fields(labs, samples, lin_spl, header_dict, format_dict):\n",
    "    gts_out = [{} for l in labs]\n",
    "    missing_samps = []\n",
    "    for s in samples:\n",
    "        d = lin_spl[header_dict[s]].split(':')\n",
    "        # cases where we don't have format info for all categories\n",
    "        if len(d) == 1:\n",
    "            if d[0] == '.':\n",
    "                missing_samps.append(s)\n",
    "        \n",
    "        for i,l in enumerate(labs):\n",
    "            if len(d) == 1:\n",
    "                if d[0] == '.':\n",
    "                    gt = './.'\n",
    "                \n",
    "            else:\n",
    "                gt = d[format_dict[l]]\n",
    "                if gt in ['.', './.']:\n",
    "                    gt = './.'\n",
    "                    missing_samps.append(s)\n",
    "                \n",
    "                \n",
    "            \n",
    "            gts_out[i][s] = gt\n",
    "    missing_samps = list(set(missing_samps))\n",
    "    return gts_out, missing_samps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def safe_div(x, y, alt=0):\n",
    "    try:\n",
    "        return x/y\n",
    "    except:\n",
    "        return alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_max_min_dict(d, samples):\n",
    "    v = [d[s] for s in samples]\n",
    "    v = [float(i) for i in v if i != './.']\n",
    "    max_v = round(max(v),4)\n",
    "    min_v = round(min(v), 4)\n",
    "    \n",
    "    return max_v, min_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_counts_to_per_sample(d, samples, cumulative_dict):\n",
    "    \n",
    "    for s in samples:\n",
    "        c = str(d[s])\n",
    "        \n",
    "        if c == ['0/0'] :\n",
    "            cat = 'REF'\n",
    "        \n",
    "        elif c in ['./.', '.']:\n",
    "            cat = 'MISSING'\n",
    "        \n",
    "        else:\n",
    "            cat = 'NREF'\n",
    "        \n",
    "        cumulative_dict[s][cat] = cumulative_dict[s].get(cat, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_counts_to_per_burden(d, samples, cumulative_dict, length, cat):  \n",
    "    copies = {'0/0':0, '0/1': 1, '1/1':2, './.':0, '.':0, '1/0': 1}\n",
    "    for s in samples:\n",
    "        c = str(d[s])\n",
    "        try:\n",
    "            bp = length * copies[c]\n",
    "        except:\n",
    "            print set(d.values())\n",
    "            return 'FAIL'\n",
    "            break\n",
    "        cumulative_dict[cat][s]['Burden'] = cumulative_dict[cat][s].get('Burden', 0) + bp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pair_concordance(pair_samples, data_dict):\n",
    "         \n",
    "    number_with_var = 0\n",
    "    number_concordant_with_var = 0\n",
    "    number_discordant_with_var = 0\n",
    "    number_missing = 0\n",
    "    per_pair_data = []\n",
    "    for tp in pair_samples:\n",
    "\n",
    "        vals =  map(str, [data_dict[tp[0]], data_dict[tp[1]]])\n",
    "\n",
    "        # check one way- first twin- for variant\n",
    "        if vals[0] == './.':\n",
    "            number_missing +=1\n",
    "            per_pair_data.append('MISSING')\n",
    "        \n",
    "        if vals[0] not in ['0/0', './.', '0']:\n",
    "\n",
    "            number_with_var +=1\n",
    "            if vals[0] == vals[1]:\n",
    "#                 tps_concordant.append(\"_\".join(tp))\n",
    "                number_concordant_with_var +=1\n",
    "                per_pair_data.append('CONCORDANT')\n",
    "            else:\n",
    "#                 tps_discordant.append(\"_\".join(tp))\n",
    "                number_discordant_with_var +=1 \n",
    "                per_pair_data.append('DISCORDANT')\n",
    "        elif vals[0] in ['0/0', '0']:\n",
    "            per_pair_data.append('REF')\n",
    "\n",
    "    if number_with_var > 0:\n",
    "        # replication rate- ranges from 0-100% 100% means every variant is a match\n",
    "        replication_rate = 1 - (number_discordant_with_var/number_with_var)\n",
    "    else:\n",
    "        replication_rate = 'None'\n",
    "    \n",
    "    if number_with_var > 0:\n",
    "        return [per_pair_data, number_with_var, number_concordant_with_var, number_discordant_with_var,\n",
    "                number_missing, replication_rate]\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_alt_allele_freq(gts_dict, samples, chrom, num_samples):\n",
    "    \"\"\" calculate alt allele freq \"\"\"\n",
    "    \n",
    " \n",
    "    #uuids containing the 1 allele\n",
    "    non_ref = 0\n",
    "    non_ref_uuids = []\n",
    "\n",
    "    #uuids containing the 0 allele \n",
    "    ref = 0\n",
    "    ref_uuids = []\n",
    "\n",
    "\n",
    "    # for chroms that are diploid\n",
    "    # correct for allelic probs on sex chroms\n",
    "\n",
    "    missing = 0\n",
    "    alleles_dist = {}\n",
    "    \n",
    "    for s in samples:\n",
    "        c = str(gts_dict[s])\n",
    "        \n",
    "        alleles_dist[c] = alleles_dist.get(c, 0) + 1\n",
    "        \n",
    "        \n",
    "        if c == './.':\n",
    "            missing +=1\n",
    "        \n",
    "        if c not in ['0|0','./.', '0']:\n",
    "            non_ref +=1\n",
    "            non_ref_uuids.append(s)\n",
    "\n",
    "        if c in ['0|0', '0']:\n",
    "            ref +=1\n",
    "            ref_uuids.append(s)\n",
    "\n",
    "\n",
    "    percent_missing = round((missing/num_samples), 3)\n",
    "    \n",
    "    if (ref == 0) & (non_ref > 0): \n",
    "        NNREF_Freq = 1.0\n",
    "          \n",
    "    else:\n",
    "        NNREF_Freq = round(safe_div(non_ref, ref, alt= 0), 4)\n",
    "    \n",
    "\n",
    "    \n",
    "    out_names = ['NREF', 'REF', 'NMissing', 'NNREF_UUIDs','NNREF_AF', 'ALLELES_DIST', 'PERC_MISSING']\n",
    "    \n",
    "    \n",
    "    alleles_dist = \",\".join([\"{}:{}\".format(i,k) for i,k in alleles_dist.iteritems()])\n",
    "    out_data = [non_ref, ref, missing, \",\".join(non_ref_uuids), NNREF_Freq, alleles_dist, percent_missing]\n",
    "    data_dict = dict(zip(out_names, out_data))\n",
    "    return data_dict, out_names, out_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prep_per_sample(df):\n",
    "    df = df.copy()\n",
    "    df = df.T\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify_snv_indel(REF, ALT):\n",
    "    \n",
    "    len_ref = len(REF)\n",
    "    len_alt = len(ALT)\n",
    "    \n",
    "    variant_type = 'SNV'\n",
    "    \n",
    "    if ALT == '*':\n",
    "        variant_type = 'DEL'\n",
    "        bp_change = len_ref\n",
    "        \n",
    "    elif len_ref == len_alt:\n",
    "        bp_change = 0\n",
    "    \n",
    "    elif len_alt > len_ref:\n",
    "        variant_type = 'INS'\n",
    "        bp_change = len_alt - len_ref\n",
    "    \n",
    "    elif len_ref > len_alt:\n",
    "        \n",
    "        variant_type = 'DEL'\n",
    "        bp_change = len_ref - len_alt\n",
    "        \n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return variant_type, bp_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prep_per_pair(df):\n",
    "    df = df.copy()\n",
    "    df = df.T\n",
    "    df['RR'] = df['CONCORDANT'] / (df['CONCORDANT'] + df['DISCORDANT'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_vcf_and_generate_qc_info(fn, out_dir, ipscore_samples, pair_samples, suff = 'auto', chroms = False):\n",
    "    \n",
    "    \n",
    "    count = 0\n",
    "    progress_level = 0\n",
    "    progress_increment = 100000\n",
    "    \n",
    "    flattened_pairs = list(set([i for sublist in pair_samples for i in sublist]))\n",
    "    \n",
    "    num_samples = len(ipscore_samples)\n",
    "    if fn.split('.').pop() == 'gz':\n",
    "        if not chroms:\n",
    "            chroms = [str(i) for i in range(1,23)] + ['X', 'Y']\n",
    "        \n",
    "        command = \"bcftools view -r {} {}\".format(\",\".join(chroms), fn)\n",
    "        F = subprocess.Popen(command, shell=True, stdout= subprocess.PIPE).stdout\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        F = open(fn, 'rU')\n",
    "        \n",
    "    if not suff:\n",
    "        replication_fn = os.path.join(out_dir, 'replication_gt_info.tsv'.format(suff))\n",
    "    else:\n",
    "        replication_fn = os.path.join(out_dir, 'replication_gt_info.{}.tsv'.format(suff))\n",
    "    replication_file = open(replication_fn, 'w')\n",
    "    \n",
    "#     per_pair_data_cumulative_all = {\"_\".join(tp):{'CONCORDANT':0, 'DISCORDANT':0, 'REF':0, 'MISSING':0} for tp in pair_samples}\n",
    "#     per_pair_data_cumulative_ins_gt = {\"_\".join(tp):{'CONCORDANT':0, 'DISCORDANT':0, 'REF':0, 'MISSING':0} for tp in pair_samples}\n",
    "    \n",
    "#     per_pair_data_cumulative_del_gt = {\"_\".join(tp):{'CONCORDANT':0, 'DISCORDANT':0, 'REF':0, 'MISSING':0} for tp in pair_samples}\n",
    "    \n",
    "    \n",
    "#     per_pair_dicts = {'SNV': per_pair_data_cumulative_snv_gt, 'DEL': per_pair_data_cumulative_del_gt,\n",
    "#                       'INS': per_pair_data_cumulative_ins_gt}\n",
    "    \n",
    "    \n",
    "\n",
    "#     overall_count_dict = {}\n",
    "    \n",
    "    count_missing = 0\n",
    "    num_variants = 0\n",
    "    d = datetime.datetime.now()\n",
    "    ts = d.strftime('%D- %H:%M')\n",
    "    print \"Starting Variant Processing: {}\".format(ts)\n",
    "    \n",
    "    h = True\n",
    "    for line in F:\n",
    "\n",
    "        line = line.rstrip()\n",
    "        lin_spl = line.split()\n",
    "        if h:\n",
    "            count +=1       \n",
    "            if line.find('#CHROM') == 0:\n",
    "                h = False\n",
    "                print 'Encountered Header End'\n",
    "                header_count = copy.deepcopy(count)\n",
    "                header = copy.deepcopy(lin_spl)\n",
    "                header_dict = {l:i for l,i in zip(lin_spl, range(0, len(lin_spl)))}\n",
    "    #             print header_dict\n",
    "\n",
    "                samples = header[9:]\n",
    "                info_cols = header[:9]\n",
    "\n",
    "#                 per_sample_data_snv_cumulative = {s:{} for s in samples}\n",
    "#                 per_sample_data_del_cumulative = {s:{} for s in samples}\n",
    "#                 per_sample_data_ins_cumulative = {s:{} for s in samples}\n",
    "#                 per_sample_dicts = {'SNV': per_sample_data_snv_cumulative, 'DEL': per_sample_data_del_cumulative,\n",
    "#                           'INS': per_sample_data_ins_cumulative}\n",
    "                \n",
    "#                 per_sample_dicts_noxy = copy.deepcopy(per_sample_dicts)\n",
    "#                 per_sample_burden_dicts_noxy = copy.deepcopy(per_sample_dicts)\n",
    "\n",
    "\n",
    "                # may not use these\n",
    "                cols_af =  ['NREF', 'REF', 'NMissing', 'NNREF_UUIDs','NNREF_AF', 'ALLELES_DIST', 'FRAC_MISSING']\n",
    "                cols_af_i2QTL_unrel = [\"{}_i2QTL_unrel\".format(i) for i in cols_af]    \n",
    "\n",
    "                cols_replication = ['CHROM', 'POS', 'ID', 'REF', 'ALT', 'NP_VAR', 'NP_CONC', 'NP_DISC', 'NP_MISSING', 'RR']\n",
    "                indel_info_header = ['CHROM', 'POS', 'ID', 'VARIANT_TYPE', 'BP_CHANGE']\n",
    "                \n",
    "\n",
    "\n",
    "                replication_file_header = (cols_replication)\n",
    "                \n",
    "                replication_file.write(\"\\t\".join(replication_file_header) + '\\n')\n",
    "#                 replication_snv_file.write(\"\\t\".join(replication_file_header) + '\\n')\n",
    "#                 indel_info_file.write(\"\\t\".join(indel_info_header) + '\\n')\n",
    "                \n",
    "            else:\n",
    "                continue\n",
    "\n",
    "\n",
    "        else:\n",
    "      \n",
    "            if progress_level == progress_increment:\n",
    "                \n",
    "                d = datetime.datetime.now()\n",
    "                ts = d.strftime('%D- %H:%M')\n",
    "                print \"processed {} variants {}\".format(num_variants, ts)\n",
    "                progress_level = 0\n",
    "\n",
    "            progress_level +=1\n",
    "            \n",
    "            count +=1\n",
    "\n",
    "            format_fields = lin_spl[header_dict['FORMAT']].split(':')\n",
    "            format_dict = {l:i for l,i in zip(format_fields, range(0, len(format_fields)))}\n",
    "\n",
    "            num_variants +=1\n",
    "\n",
    "            info_col = header_dict['INFO']\n",
    "\n",
    "            POS = lin_spl[header_dict['POS']]\n",
    "            ID = lin_spl[header_dict['ID']]\n",
    "            info = lin_spl[header_dict['INFO']]\n",
    "            chrom = str(lin_spl[header_dict['#CHROM']])\n",
    "            REF = lin_spl[header_dict['REF']]\n",
    "            ALT = lin_spl[header_dict['ALT']]\n",
    "#             variant_type, bp_change = classify_snv_indel(REF, ALT)\n",
    "            \n",
    "#             if variant_type != 'SNV':\n",
    "#                 out = [chrom, POS, ID, variant_type, bp_change]\n",
    "#                 indel_info_file.write(\"\\t\".join(map(str, out)) + '\\n')\n",
    "            \n",
    "#             overall_count_dict[variant_type] = overall_count_dict.get(variant_type, 0) + 1\n",
    "            \n",
    "            \n",
    "            fields_desired = ['GT']\n",
    "            geno_fields_dict = {fields_desired[i]:i for i in range(0, len(fields_desired))}\n",
    "\n",
    "            geno_extracted, missing_samps = get_geno_fields(fields_desired, samples, lin_spl, \n",
    "                                                            header_dict, format_dict)\n",
    "            gt_dict = geno_extracted[geno_fields_dict['GT']]\n",
    "             \n",
    "            \n",
    "            subset_missing = [i for i in missing_samps if i in ipscore_samples]\n",
    "            num_missing = len(subset_missing)\n",
    "\n",
    "            num_passing = num_samples - num_missing\n",
    "            \n",
    "\n",
    "\n",
    "            # ======= Add info to cumulative dict ==========\n",
    "#             bp = 1\n",
    "#             if variant_type != 'SNV':\n",
    "#                 bp = bp_change\n",
    "                \n",
    "#             add_counts_to_per_sample(gt_dict, samples,  per_sample_dicts[variant_type])\n",
    "\n",
    "            #====== INFO Col extractions ========================\n",
    "\n",
    "\n",
    "#             cols_dict = dict(zip(cols_info_extracted, range(0, len(cols_info_extracted))))\n",
    "#             info_out = []\n",
    "#             info_cols_dict = {}\n",
    "#             for l in cols_info_extracted:\n",
    "#                 name_field = col_name_dict[l]\n",
    "#                 c = parse_info_col(info, name_field)\n",
    "#                 info_out.append(c)\n",
    "#                 info_cols_dict[name_field] = c\n",
    "            #====================================================\n",
    "            # aggregate locus stats\n",
    "\n",
    "\n",
    "\n",
    "                #================TWIN/PAIR Replication Calculations ==========\n",
    "                \n",
    "                # check for non_missing_gt\n",
    "            if chrom not in ['X', 'Y']:\n",
    "                \n",
    "#                 TEST = add_counts_to_per_burden(gt_dict, samples,  per_sample_burden_dicts_noxy, bp, variant_type)\n",
    "#                 if TEST == 'FAIL':\n",
    "#                     print REF, ALT, ID, chrom, POS\n",
    "#                     print lin_spl\n",
    "#                     break\n",
    "#                     return\n",
    "             \n",
    "                \n",
    "#                 add_counts_to_per_sample(gt_dict, samples, per_sample_dicts_noxy[variant_type])\n",
    "\n",
    "                t = [gt_dict[s] for s in flattened_pairs if gt_dict[s] != './.']\n",
    "                if len(t) > 0:\n",
    "                    pair_data_gt = calculate_pair_concordance(pair_samples, gt_dict)\n",
    "                    if pair_data_gt != False:\n",
    "\n",
    "                        (per_pair_data_gt, number_with_var, \n",
    "                         number_concordant_with_var, number_discordant_with_var,\n",
    "                         number_missing, replication_rate) = pair_data_gt\n",
    "\n",
    "                        OUT_PAIR_DATA_GT = copy.deepcopy([number_with_var, number_concordant_with_var,\n",
    "                                         number_discordant_with_var, number_missing, replication_rate])\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "#                         for tp, d_gt in zip(pair_samples, per_pair_data_gt):\n",
    "#                             p = \"_\".join(tp)\n",
    "#                             per_pair_dicts[variant_type][p][d_gt] = (per_pair_dicts[variant_type][p].get(d_gt, 0) + 1)\n",
    "                        \n",
    "                        OUT_LINE_REPLICATION_GT = ([chrom, POS, ID, REF, ALT] + OUT_PAIR_DATA_GT)\n",
    "                        \n",
    "#                         name = \"{}_seg\".format(variant_type)\n",
    "#                         overall_count_dict[name] = overall_count_dict.get(name, 0) + 1 \n",
    "                                          \n",
    "                        replication_file.write(\"\\t\".join(map(str,OUT_LINE_REPLICATION_GT)) + '\\n') \n",
    "    \n",
    "        \n",
    "    \n",
    "    replication_file.close()\n",
    "#     replication_indel_file.close()\n",
    "#     print \"saving files\"\n",
    "#     cats = ['SNV', 'INS', 'DEL']\n",
    "#     for c in cats:\n",
    "#         ppd = per_pair_dicts[c]\n",
    "#         fn_per_pair_gt = os.path.join(out_dir, '{}_per_pair_gt.{}.tsv'.format(c, suff))\n",
    "#         per_pair_gt  = pd.DataFrame(ppd).pipe(prep_per_pair)\n",
    "#         per_pair_gt.to_csv(fn_per_pair_gt, sep = '\\t')\n",
    "#         print fn_per_pair_gt\n",
    "\n",
    "#         fn_per_sample = os.path.join(out_dir, '{}_per_sample_gt.{}.tsv'.format(c, suff))\n",
    "#         psd = per_sample_dicts[c] \n",
    "#         per_sample_gt = pd.DataFrame(psd).pipe(prep_per_sample)\n",
    "#         per_pair_gt.to_csv(fn_per_sample, sep = '\\t')\n",
    "#         print fn_per_sample\n",
    "        \n",
    "#         fn_per_sample = os.path.join(out_dir, '{}_per_sample_burden_auto.{}.tsv'.format(c,suff))\n",
    "#         psd = per_sample_burden_dicts_noxy[c] \n",
    "#         per_sample_gt = pd.DataFrame(psd).pipe(prep_per_sample)\n",
    "#         per_sample_gt.to_csv(fn_per_sample, sep = '\\t')\n",
    "#         print fn_per_sample\n",
    "\n",
    "#         fn_per_sample_noxy = os.path.join(out_dir, '{}_per_sample_auto_gt.{}.tsv'.format(c, suff))\n",
    "#         psd = per_sample_dicts_noxy[c] \n",
    "#         per_sample_gt = pd.DataFrame(psd).pipe(prep_per_sample)\n",
    "#         per_pair_gt.to_csv(fn_per_sample_noxy, sep = '\\t')\n",
    "#         print fn_per_sample\n",
    "\n",
    "#     fn_overall_count =  os.path.join(out_dir, 'variant_counts.{}.tsv'.format(suff))\n",
    "#     tdf = pd.Series(overall_count_dict).to_frame('Num_Variants')\n",
    "#     tdf['VARIANT_TYPE'] = tdf.index\n",
    "#     tdf.to_csv(fn_overall_count, sep = '\\t', index =False, header = False)\n",
    "\n",
    "    print \"number of variants: {}\".format(num_variants)\n",
    "    d = datetime.datetime.now()\n",
    "    ts = d.strftime('%D- %H:%M')\n",
    "    print \"Completed Variant Processing: {}\".format(ts)\n",
    "    print \"replication info: {}\".format(replication_fn)\n",
    "    \n",
    "    print 'COMPLETE'\n",
    "    \n",
    "    return\n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_arguments_to_parser(parser):\n",
    "    \n",
    "\n",
    "    parser.add_argument(\"-vcf\", \"--vcf\", dest=\"vcf_file\", metavar='<vcf_file>', help=\"vcf file from lumpy/speedseq pipeline, may be gzipped or not\", required=True)\n",
    "    \n",
    "    parser.add_argument(\"-pairs\", \"--pairs\", dest=\"pairs_fn\", metavar='<pair_fn>', help=\"file of pairs of samples, likely genetically duplicate samples (twins, replicates, biological replicates) (tsv with no header)\", required=True)\n",
    "    \n",
    "    parser.add_argument(\"-s\", \"--samples\", dest=\"samples_fn\", metavar='<samples_fn>', help=\"list of samples on which to compute MAF and other general stats\", required=True)\n",
    "    \n",
    "    \n",
    "    parser.add_argument(\"-suff\", \"--suff\", dest=\"suffix\", metavar='<suffix>', help=\"suffix of info columns\", required=False, default = False)\n",
    "        \n",
    "    parser.add_argument(\"-chroms\", \"--chroms\", dest=\"chroms\", metavar='<suffix>', help=\"chroms to process\", required=False, default = False)\n",
    "  \n",
    "    \n",
    "    parser.add_argument(\"-o\", \"--output_dir\", dest=\"output_dir\", metavar='<out_dir>', help=\"output directory for summary output\", required=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    parser.set_defaults(entry_point=run_from_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def command_parser():\n",
    "    parser = argparse.ArgumentParser(description= 'command line utility to extract replication and length information for SNV and indels')\n",
    "    add_arguments_to_parser(parser)\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_from_args(args):\n",
    "    vcf_fn = args.vcf_file\n",
    "    samples_fn = args.samples_fn\n",
    "    suff = args.suffix\n",
    "    pair_fn = args.pairs_fn\n",
    "    out_dir = args.output_dir\n",
    "    pairs = [line.rstrip().split() for line in open(pair_fn)]\n",
    "    samples = [line.rstrip() for line in open(samples_fn)]   \n",
    "    chroms = args.chroms\n",
    "    if chroms:\n",
    "        chroms = chroms.split(\",\")\n",
    "    process_vcf_and_generate_qc_info(vcf_fn, out_dir, samples, pairs, suff = suff, chroms = chroms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = command_parser()\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    sys.exit(args.entry_point(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fn = '/frazer01/projects/hipsci/pipeline/WGS/HipSTR/combined_results/hipstr_ipscore.vcf'\n",
    "\n",
    "# out_dir = '/frazer01/projects/hipsci/analysis/i2QTL-sv-analysis/private_output/hipstr_qc_analysis/test'\n",
    "\n",
    "\n",
    "\n",
    "# twin_fn = '/frazer01/projects/hipsci/analysis/i2QTL-sv-analysis/private_output/ipscore_sample_info/twins.tsv'\n",
    "# pairs = [line.rstrip().split() for line in open(twin_fn)]    \n",
    "\n",
    "# fn_ipscore = '/frazer01/projects/hipsci/analysis/i2QTL-sv-analysis/private_output/sample_info_combined/samples_ipscore.txt'\n",
    "\n",
    "# samples = [line.rstrip() for line in open(fn_ipscore)]\n",
    "\n",
    "# p,d = process_vcf_and_generate_qc_info(fn, out_dir, samples, pairs)\n",
    "\n",
    "# fn = '/frazer01/projects/hipsci/analysis/i2QTL-sv-analysis/private_output/hipstr_qc_analysis/ipscore_geno/hipstr_per_sample_length.tsv'\n",
    "\n",
    "# fn =  '/frazer01/projects/hipsci/analysis/i2QTL-sv-analysis/private_output/hipstr_qc_analysis/test/hipstr_per_sample_gt.tsv'\n",
    "\n",
    "# fn =  '/frazer01/projects/hipsci/analysis/i2QTL-sv-analysis/private_output/hipstr_qc_analysis/test/hipstr_per_sample_length.tsv'\n",
    "\n",
    "# t = pd.read_table(fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
