{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "from subprocess import call\n",
    "import subprocess\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "import copy \n",
    "\n",
    "\n",
    "import itertools\n",
    "import tempfile\n",
    "import six\n",
    "import networkx as nx\n",
    "import scipy.stats as stats\n",
    "import argparse\n",
    "\n",
    "import datetime\n",
    "import gzip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_header_end(fn):\n",
    "    \"\"\" find header end of vcf or gzipped vcf\"\"\"\n",
    "    \n",
    "    if fn.split('.').pop() == 'gz':\n",
    "        import gzip\n",
    "        F = gzip.open(fn, 'rU')\n",
    "    else:\n",
    "        F = open(fn, 'rU')\n",
    "        \n",
    "\n",
    "    count = 0\n",
    "    for line in F:\n",
    "        count +=1\n",
    "        try: \n",
    "            spl = line.split('\\t')\n",
    "            spl0 = spl[0] \n",
    "            if spl[0]==\"#CHROM\":\n",
    "                F.close()\n",
    "                return count\n",
    "            if count > 2000:\n",
    "                return 'incomplete or missing header, or very long header'\n",
    "        except:\n",
    "            break\n",
    "    \n",
    "    F.close()\n",
    "\n",
    "def parse_info_col(t, lab, type_out = str, alternative = 'None'):\n",
    "    \"\"\" parse the info column\"\"\"\n",
    "    \n",
    "    t = t.split(';')\n",
    "    \n",
    "    # remove any of the tags that have no label\n",
    "    t = [i for i in t if i.find('=') != -1]\n",
    "    \n",
    "    cols = [i.split('=')[0] for i in t]\n",
    "    try:\n",
    "        vals = [i.split('=')[1] for i in t]    \n",
    "    except:\n",
    "        return \"PARSE_ERROR\"\n",
    "    try:\n",
    "        ind = cols.index(lab)\n",
    "        v = vals[ind]\n",
    "        \n",
    "        try:\n",
    "            return type_out(v)\n",
    "        except:\n",
    "            return alternative\n",
    "    except:\n",
    "        return 'Column_Not_Present'\n",
    "\n",
    "def bp_precision(x):\n",
    "    beg, end =[int(i) for i in x.split(',')]\n",
    "    interval = abs(end - beg)\n",
    "    return interval\n",
    "\n",
    "\n",
    "def SVLEN_convert(x):\n",
    "    try:\n",
    "        return np.abs(int(x))\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def parse_format_col(t, col, lab, type_out = str, alternative = 'None', change_dict = False):\n",
    "    \"\"\" use the format column to pull out info of a specific VCF record\n",
    "    in a genotyping matrix (unparsed)\n",
    "    change_dict- dict to return something else for any given value in the format col\n",
    "    \"\"\"\n",
    "    \n",
    "    f= t['FORMAT']\n",
    "    c = t[col]\n",
    "    c  = c.split(':')\n",
    "    f = f.split(':')\n",
    "    \n",
    "    try:\n",
    "        ind = f.index(lab)\n",
    "        v = c[ind]\n",
    "        try:\n",
    "            if change_dict:\n",
    "                try:\n",
    "                    v = change_dict.get(v, v)\n",
    "                except:\n",
    "                    return 'ERROR'\n",
    "            \n",
    "            return type_out(v)\n",
    "        except:\n",
    "            return alternative\n",
    "            \n",
    "    except:\n",
    "        return 'Column_Not_Present'\n",
    "    \n",
    "\n",
    "def parse_format_mod(t, col, lab, except_out = 'None'):\n",
    "    \"\"\" use the format column to pull out info of a specific VCF record\n",
    "    in a genotyping matrix (unparsed)\"\"\"\n",
    "    \n",
    "    f= t['FORMAT']\n",
    "    c = t[col]\n",
    "    c  = c.split(':')\n",
    "    f = f.split(':')\n",
    "    \n",
    "    try:\n",
    "        ind = f.index(lab)\n",
    "        v = c[ind]\n",
    "        return v\n",
    "    except:\n",
    "        return except_out\n",
    "\n",
    "\n",
    "def coord_extract(df,chrom, start, end, contained = True):\n",
    "    \n",
    "    if contained:\n",
    "        return df[(df.Chr==chrom) & (df.POS >= start) & (df.END <= end)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_annotations_extract_gts(fn, out_dir, unrelated_samples, non_ipsc_samples, sex_dict):\n",
    "    \n",
    "    header_end = find_header_end(fn)\n",
    "    \n",
    "    count = 0\n",
    "    if fn.split('.').pop() == 'gz':\n",
    "        F = gzip.open(fn, 'rU')\n",
    "    else:\n",
    "        F = open(fn, 'rU')\n",
    "    \n",
    "    \n",
    "    gt_file = open(os.path.join(out_dir, 'melt_gt.tsv'), 'w')\n",
    "    info_file = open(os.path.join(out_dir, 'melt_info.tsv'), 'w')\n",
    "    \n",
    "    fixed_header_vcf = open(os.path.join(out_dir, 'melt_final.vcf'), 'w')\n",
    "    filtered_vcf = open(os.path.join(out_dir, 'melt_final_filt.vcf'), 'w')\n",
    "    \n",
    "    gt_filt_file = open(os.path.join(out_dir, 'melt_gt_filt.tsv'), 'w')\n",
    "#     ab_filt_file = open(os.path.join(out_dir, 'lumpy_ab_filt.tsv'), 'w')\n",
    "    \n",
    "    info_filt_file = open(os.path.join(out_dir, 'melt_info_filt.tsv'), 'w')\n",
    "    \n",
    "    chroms = [str(i) for i in range(1,23)] + ['X', 'Y']\n",
    "    \n",
    "    add_filters = False\n",
    "    add_info = False\n",
    "    for line in F:\n",
    "        line = line.rstrip()\n",
    "                         \n",
    "        lin_spl = line.split('\\t')\n",
    "                         \n",
    "        if count < header_end-1:\n",
    "            if not add_info:\n",
    "                if line.find('##INFO') == 0:\n",
    "                    add_info = True\n",
    "                    l= '##INFO=<ID=1KGP_ID,Number=1,Type=String,Description=\"ID indicating site found previously in 1KGP (site included in priors)\">'\n",
    "                    fixed_header_vcf.write(l + '\\n')\n",
    "                    filtered_vcf.write(l + '\\n')\n",
    "            \n",
    "            if not add_filters:\n",
    "                if line.find('##FILTER') == 0:\n",
    "                    add_filters = True\n",
    "\n",
    "                    l2= '##FILTER=<ID=iPSC,Description=\"Filter indicating site present only in iPSC\">'\n",
    "                    l1 = '##FILTER=<ID=ASSESS,Description=\"Filter indicating site has ASSESS tranche less than 5\">'\n",
    "                    l3 = '##FILTER=<ID=NMissing,Description=\"at least 10% of samples have a missing GT (iPSCORE fib/blood + HipSci fib)>'\n",
    "                    l4 =  '##FILTER=<ID=OTHER_CONTIG,Description=\"not on autosomes or sex chroms\">'\n",
    "                    \n",
    "                    l = \"\\n\".join([l1, l2, l3, l4])\n",
    "                    \n",
    "                    fixed_header_vcf.write(l + '\\n') \n",
    "                    filtered_vcf.write(l + '\\n')\n",
    "                    \n",
    "            fixed_header_vcf.write(line + '\\n')\n",
    "            filtered_vcf.write(line + '\\n')\n",
    "        \n",
    "                  \n",
    "        if count == header_end-1:\n",
    " \n",
    "            # fix the .mdup issue in the sample naming from processing\n",
    "            \n",
    "            lin_spl = [i.replace('.mdup','') for i in lin_spl]\n",
    "            header_dict = {l:i for l,i in zip(lin_spl, range(0, len(lin_spl)))}\n",
    "            \n",
    "            samples = lin_spl[9:]\n",
    "            info_cols = lin_spl[:9]\n",
    "            \n",
    "            cols_maf = ['NNREF', 'NNREF_UUIDs','ALTAF', 'REFAF', 'MAF', 'Minor_Allele', 'NREF', 'UUIDS_REF',\n",
    "                        'NMissing']\n",
    "            cols_extracted = ['SVTYPE', 'INTERNAL', 'RP', 'LP', 'SR', 'RA', 'SVLEN', 'ASSESS', 'TSD','MEINFO'] \n",
    "            cols_maf_unr = [i+ '_unr' for i in cols_maf]\n",
    "            cols_maf_non_ipsc = [i+ '_non_ipsc' for i in cols_maf]\n",
    "            \n",
    "            #remove the info column for space savings\n",
    "        \n",
    "            \n",
    "            info_header = ['NAME'] +info_cols[:7] + cols_extracted + ['END','Missing_GTs'] + cols_maf + cols_maf_unr + cols_maf_non_ipsc + ['FILTER_ORIGINAL'] + ['ID_ORIGINAL']\n",
    "            \n",
    "            genotypes_header = ['NAME'] + samples\n",
    "            \n",
    "            gt_file.write(\"\\t\".join(genotypes_header) + '\\n')\n",
    "            gt_filt_file.write(\"\\t\".join(genotypes_header) + '\\n')\n",
    "            \n",
    "            info_file.write(\"\\t\".join(info_header) + '\\n')\n",
    "            info_filt_file.write(\"\\t\".join(info_header) + '\\n')\n",
    "            \n",
    "            fixed_header_vcf.write( \"\\t\".join(lin_spl) + '\\n')\n",
    "            filtered_vcf.write( \"\\t\".join(lin_spl) + '\\n')\n",
    "            \n",
    "            \n",
    "        elif count > header_end:\n",
    "            \n",
    "            if count == header_end+1:\n",
    "            \n",
    "                format_fields = lin_spl[header_dict['FORMAT']].split(':')\n",
    "                format_dict = {l:i for l,i in zip(format_fields, range(0, len(format_fields)))}\n",
    "                  \n",
    "            \n",
    "            sample_gts = []\n",
    "            # original first 9 columns\n",
    "            \n",
    "            info_col = header_dict['INFO']\n",
    "            \n",
    "            \n",
    "            first_cols = lin_spl[:7]\n",
    "            \n",
    "            POS = lin_spl[header_dict['POS']]\n",
    "            FILTER = lin_spl[header_dict['FILTER']]\n",
    "            FILTER_ORIGINAL = copy.deepcopy(FILTER)\n",
    "            \n",
    "            ID = lin_spl[header_dict['ID']]\n",
    "            ALT = lin_spl[header_dict['ALT']]\n",
    "            vtype = ALT.split(':')[-1][:-1]\n",
    "            info = lin_spl[header_dict['INFO']]\n",
    "            chrom = lin_spl[header_dict['#CHROM']]\n",
    "            \n",
    "            # new ID\n",
    "#             ID_MOD = \"{}_{}_{}\".format(vtype, chrom, POS)\n",
    "            \n",
    "            \n",
    "    \n",
    "            cols_to_parse =  ['SVTYPE', 'INTERNAL', 'RP', 'LP', 'SR', 'RA', 'SVLEN', 'ASSESS', 'TSD', 'MEINFO'] \n",
    "            cols_dict = dict(zip(cols_to_parse, range(0, len(cols_to_parse))))\n",
    "            \n",
    "            info_out = []\n",
    "            \n",
    "            info_cols_dict = {}\n",
    "            for l in cols_to_parse:\n",
    "                c = parse_info_col(info, l)\n",
    "                info_out.append(c)\n",
    "                info_cols_dict[l] = c\n",
    "\n",
    "            ASSESS = int(info_cols_dict['ASSESS'])\n",
    "            \n",
    "            # give the variants more useful IDs \n",
    "            END = int(POS) + 1\n",
    "            ID_MOD = \"{}_{}_{}_{}\".format(vtype, chrom, POS, END)\n",
    "            \n",
    "            \n",
    "            if ID != '.':\n",
    "                lin_spl[header_dict['INFO']] += ';1KGP_ID={}'.format(ID)\n",
    "                \n",
    "                # swap in my ID because its more useful- but add in the 1KGP_ID to the info column\n",
    "                lin_spl[header_dict['ID']] = ID_MOD\n",
    "                \n",
    "            else:\n",
    "                lin_spl[header_dict['ID']] = ID_MOD\n",
    "                \n",
    "            \n",
    "            \n",
    "            # add minor allele frequency:\n",
    "            \n",
    "            gts_out = []\n",
    "            \n",
    "            # all samples in this case\n",
    "            \n",
    "            for s in samples:\n",
    "                d = lin_spl[header_dict[s]].split(':')\n",
    "                gt = d[format_dict['GT']]\n",
    "                gts_out.append(gt)\n",
    "            \n",
    "            \n",
    "            missing_gts = ('./.' in gts_out)\n",
    "          \n",
    "            \n",
    "            # minor allele frequencies\n",
    "            maf_data_all = calculate_minor_allele_freq(gts_out, samples, chrom, sex_dict)\n",
    "        \n",
    "            s = [gts_out[samples.index(i)] for i in unrelated_samples]\n",
    "            maf_data_unr = calculate_minor_allele_freq(s, unrelated_samples, chrom, sex_dict)\n",
    " \n",
    "            s = [gts_out[samples.index(i)] for i in non_ipsc_samples] \n",
    "            maf_data_non_ipsc = calculate_minor_allele_freq(s, non_ipsc_samples, chrom, sex_dict)\n",
    "            \n",
    "            # write out data\n",
    "            num_missing_non_ipsc = int(maf_data_non_ipsc[8])\n",
    "#             if ID_MOD == 'ALU_1_24328785_24328786':\n",
    "#                 print num_missing_non_ipsc\n",
    "            perc_missing = (num_missing_non_ipsc/478)\n",
    "            chroms = [str(i) for i in range(1,23)] + ['X', 'Y']\n",
    "        \n",
    "        \n",
    "            if str(chrom) not in chroms:\n",
    "                if FILTER == 'PASS':\n",
    "                    FILTER_MOD = 'OTHER_CONTIG'\n",
    "                else:\n",
    "                    FILTER_MOD = FILTER + ';OTHER_CONTIG'\n",
    "\n",
    "                lin_spl[header_dict['FILTER']] = FILTER_MOD\n",
    "                FILTER = FILTER_MOD\n",
    "                \n",
    "            \n",
    "            if (perc_missing > 0.1):\n",
    "                if FILTER == 'PASS':\n",
    "                    FILTER_MOD = 'NMissing'\n",
    "                else:\n",
    "                    FILTER_MOD = FILTER + ';NMissing'\n",
    "                \n",
    "                lin_spl[header_dict['FILTER']] = FILTER_MOD\n",
    "                FILTER = FILTER_MOD\n",
    "                \n",
    "            \n",
    "            \n",
    "            if ASSESS < 5:\n",
    "                if FILTER == 'PASS':\n",
    "                    FILTER_MOD = 'ASSESS'\n",
    "                else:\n",
    "                    FILTER_MOD = FILTER + ';ASSESS'\n",
    "                \n",
    "                lin_spl[header_dict['FILTER']] = FILTER_MOD\n",
    "                FILTER = FILTER_MOD\n",
    "                \n",
    "            \n",
    "            \n",
    "            if int(maf_data_non_ipsc[0]) == 0:\n",
    "                if FILTER == 'PASS':\n",
    "                    FILTER_MOD = 'iPSC'\n",
    "                else:\n",
    "                    FILTER_MOD = FILTER + ';iPSC'\n",
    "            \n",
    "                lin_spl[header_dict['FILTER']] = FILTER_MOD\n",
    "                \n",
    "                FILTER = FILTER_MOD\n",
    "            \n",
    "            \n",
    "            first_cols = lin_spl[:7]\n",
    "            out_line_info =  [ID_MOD] + first_cols + info_out + [END] + [missing_gts] + maf_data_all + maf_data_unr + maf_data_non_ipsc + [FILTER_ORIGINAL] + [ID]\n",
    "            out_line_info = map(str, out_line_info)\n",
    "            out_line_gt = [ID_MOD] + gts_out\n",
    "            \n",
    "            # filtered_data\n",
    "            if FILTER == 'PASS':\n",
    "                info_filt_file.write(\"\\t\".join(out_line_info) + '\\n')\n",
    "                gt_filt_file.write(\"\\t\".join(out_line_gt) + '\\n')\n",
    "                filtered_vcf.write(\"\\t\".join(lin_spl) + '\\n')\n",
    "                \n",
    "            \n",
    "            info_file.write(\"\\t\".join(out_line_info) + '\\n')\n",
    "            \n",
    "            gt_file.write(\"\\t\".join(out_line_gt) + '\\n')\n",
    "            \n",
    "            fixed_header_vcf.write(\"\\t\".join(lin_spl) + '\\n')\n",
    "           \n",
    "        \n",
    "        \n",
    "        count +=1\n",
    "    \n",
    "    # close out files\n",
    "    F.close()\n",
    "    gt_file.close() \n",
    "    fixed_header_vcf.close()\n",
    "    info_file.close()\n",
    "    gt_filt_file.close()\n",
    "    info_filt_file.close()\n",
    "    filtered_vcf.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def safe_div(x, y, alt=0):\n",
    "    try:\n",
    "        return x/y\n",
    "    except:\n",
    "        return alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_minor_allele_freq(gts, uuids, chrom, sex_dict):\n",
    "    \"\"\" calculate minor allele frequency, assume missing genotypes are reference\"\"\"\n",
    "    \n",
    "\n",
    "    #uuids containing the 1 allele\n",
    "    non_ref = 0\n",
    "    non_ref_uuids = []\n",
    "\n",
    "    #uuids containing the 0 allele \n",
    "    ref = 0\n",
    "    ref_uuids = []\n",
    "\n",
    "    ref_allele = 0\n",
    "    alt_allele = 0\n",
    "\n",
    "    # for chroms that are diploid\n",
    "    # correct for allelic probs on sex chroms\n",
    "    gts_dict = {'0/0':0, '0/1':1, '1/1':2, './.':0}\n",
    "    gts_dict_sex = {'0/0':0, '0/1':1, '1/1':1, './.':0}\n",
    "    missing = 0\n",
    "    \n",
    "    count = 0\n",
    "    for c in gts:\n",
    "        uuid = uuids[count]\n",
    "        sex = sex_dict[uuid]\n",
    "        \n",
    "        if c == './.':\n",
    "            missing +=1\n",
    "            # skip onward- we are ignoring missing\n",
    "            continue\n",
    "        \n",
    "\n",
    "        if chrom=='Y':\n",
    "            if sex == 'M':\n",
    "                aa = gts_dict_sex[c]\n",
    "                ra = 1 - aa\n",
    "                \n",
    "            else:\n",
    "                count +=1\n",
    "                continue\n",
    "\n",
    "\n",
    "\n",
    "        elif chrom=='X':\n",
    "            if sex == 'M':\n",
    "                aa = gts_dict_sex[c]\n",
    "                ra = 1 - aa\n",
    "            else:\n",
    "                aa = gts_dict[c]\n",
    "                ra = 2 - aa\n",
    "\n",
    "\n",
    "        else:\n",
    "            aa = gts_dict[c]\n",
    "            ra = 2 - aa\n",
    "#                 ref_allele += ra\n",
    "#                 alt_allele += aa\n",
    "\n",
    "        ref_allele += ra\n",
    "        alt_allele += aa\n",
    "\n",
    "        if c not in ['0/0','./.']:\n",
    "            non_ref +=1\n",
    "            non_ref_uuids.append(uuids[count])\n",
    "\n",
    "        if c not in ['1/1','0/1']:\n",
    "            ref +=1\n",
    "            ref_uuids.append(uuids[count])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        count +=1\n",
    "\n",
    "\n",
    "\n",
    "    tot_alleles = ref_allele + alt_allele\n",
    "\n",
    "\n",
    "    ref_af = safe_div(ref_allele, tot_alleles, alt='All Missing')\n",
    "    alt_af = safe_div(alt_allele, tot_alleles, alt='All Missing')\n",
    "\n",
    "    afs = [ref_af, alt_af]\n",
    "    \n",
    "    \n",
    "    maf = 0.0\n",
    "    minor_allele = 'ALT'\n",
    "    if not tot_alleles == 0:\n",
    "        maf = min(afs)\n",
    "        if afs.index(maf) == 0:\n",
    "            minor_allele = 'REF'\n",
    "        else:\n",
    "            minor_allele = 'ALT'\n",
    "            \n",
    "    \n",
    "#     ref_af = ref_allele/tot_alleles\n",
    "#     alt_af = alt_allele/tot_alleles\n",
    "\n",
    "#     afs = [ref_af, alt_af]\n",
    "#     maf = min(afs)\n",
    "    \n",
    "\n",
    "#     if afs.index(maf) == 0:\n",
    "#         minor_allele = 'REF'\n",
    "#     else:\n",
    "#         minor_allele = 'ALT'\n",
    "    \n",
    "\n",
    "    out_names = ['NNREF', 'NNREF_UUIDs','ALTAF', 'REFAF', 'MAF', 'Minor_Allele', 'NREF', 'UUIDS_REF', 'Missing']\n",
    "    out_data = [non_ref, \",\".join(non_ref_uuids), alt_af, ref_af, maf, minor_allele, ref, \",\".join(ref_uuids),\n",
    "               missing]\n",
    "    \n",
    "    data_dict = dict(zip(out_names, out_data))\n",
    "    \n",
    "    \n",
    "    return out_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_arguments_to_parser(parser):\n",
    "    \n",
    "\n",
    "    parser.add_argument(\"-vcf\", \"--vcf\", dest=\"vcf_file\", metavar='<vcf_file>', help=\"vcf file from lumpy/speedseq pipeline, may be gzipped or not\", required=True)\n",
    "\n",
    "    \n",
    "    parser.add_argument(\"-gender\", \"--gender_map\", dest=\"gender_map\", metavar='<gender.txt>', help=\"gender_map_file\", required=True)\n",
    "        \n",
    "    \n",
    "    parser.add_argument(\"-unr\", \"--unrelated_samples\", dest=\"unr_samples\", metavar='<unrelated_samples.txt>', help=\"list of sample names that indicate unrelated samples\", required=True)\n",
    "    \n",
    "      \n",
    "    parser.add_argument(\"-non_ipsc\", \"--non_ipsc_samples\", dest=\"non_ipsc_samples\", metavar='<non_ipsc_samples.txt>', help=\"list of sample names that indicate samples that are not iPSC\", required=True)\n",
    "  \n",
    "    \n",
    "    parser.add_argument(\"-o\", \"--output_dir\", dest=\"output_dir\", metavar='<out_dir>', help=\"output directory for summary output\", required=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    parser.set_defaults(entry_point=run_from_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def command_parser():\n",
    "    parser = argparse.ArgumentParser(description= 'command line utility to annotate extracted info and genotypes from lumpy/melt with various things such as gene intersections, minor allele frequency etc.')\n",
    "    add_arguments_to_parser(parser)\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_from_args(args):\n",
    "\n",
    "    unrelated_fn = args.unr_samples\n",
    "    non_ipsc_fn = args.non_ipsc_samples\n",
    "\n",
    "    unr_samples = [line.rstrip() for line in open(unrelated_fn)]\n",
    "    non_ipsc_samples = [line.rstrip() for line in open(non_ipsc_fn)]\n",
    "    \n",
    "    \n",
    "    gender_fn = args.gender_map\n",
    "    sex_dict = {line.rstrip().split()[0]:line.rstrip().split()[1] for line in open(gender_fn)}\n",
    "    \n",
    "    t = datetime.datetime.now().strftime('%c')\n",
    "    print \"extracting variants, annotating filters and MAFs: \", t\n",
    "    \n",
    "    add_annotations_extract_gts(args.vcf_file, args.output_dir, unr_samples, non_ipsc_samples, sex_dict)\n",
    "    \n",
    "    t = datetime.datetime.now().strftime('%c')\n",
    "    print 'complete:', t\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = command_parser()\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    sys.exit(args.entry_point(args))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
